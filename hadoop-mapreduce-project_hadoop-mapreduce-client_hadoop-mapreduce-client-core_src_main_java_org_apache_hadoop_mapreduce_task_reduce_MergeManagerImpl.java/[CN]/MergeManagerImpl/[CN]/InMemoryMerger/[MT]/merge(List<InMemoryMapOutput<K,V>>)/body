{
  if (inputs == null || inputs.size() == 0) {
    return;
  }
  TaskAttemptID mapId=inputs.get(0).getMapId();
  TaskID mapTaskId=mapId.getTaskID();
  List<Segment<K,V>> inMemorySegments=new ArrayList<Segment<K,V>>();
  long mergeOutputSize=createInMemorySegments(inputs,inMemorySegments,0);
  int noInMemorySegments=inMemorySegments.size();
  Path outputPath=mapOutputFile.getInputFileForWrite(mapTaskId,mergeOutputSize).suffix(Task.MERGED_OUTPUT_PREFIX);
  FSDataOutputStream out=CryptoUtils.wrapIfNecessary(jobConf,rfs.create(outputPath));
  Writer<K,V> writer=new Writer<K,V>(jobConf,out,(Class<K>)jobConf.getMapOutputKeyClass(),(Class<V>)jobConf.getMapOutputValueClass(),codec,null,true);
  RawKeyValueIterator rIter=null;
  CompressAwarePath compressAwarePath;
  try {
    LOG.info("Initiating in-memory merge with " + noInMemorySegments + " segments...");
    rIter=Merger.merge(jobConf,rfs,(Class<K>)jobConf.getMapOutputKeyClass(),(Class<V>)jobConf.getMapOutputValueClass(),inMemorySegments,inMemorySegments.size(),new Path(reduceId.toString()),(RawComparator<K>)jobConf.getOutputKeyComparator(),reporter,spilledRecordsCounter,null,null);
    if (null == combinerClass) {
      Merger.writeFile(rIter,writer,reporter,jobConf);
    }
 else {
      combineCollector.setWriter(writer);
      combineAndSpill(rIter,reduceCombineInputCounter);
    }
    writer.close();
    compressAwarePath=new CompressAwarePath(outputPath,writer.getRawLength(),writer.getCompressedLength());
    LOG.info(reduceId + " Merge of the " + noInMemorySegments+ " files in-memory complete."+ " Local file is "+ outputPath+ " of size "+ localFS.getFileStatus(outputPath).getLen());
  }
 catch (  IOException e) {
    localFS.delete(outputPath,true);
    throw e;
  }
  closeOnDiskFile(compressAwarePath);
}
