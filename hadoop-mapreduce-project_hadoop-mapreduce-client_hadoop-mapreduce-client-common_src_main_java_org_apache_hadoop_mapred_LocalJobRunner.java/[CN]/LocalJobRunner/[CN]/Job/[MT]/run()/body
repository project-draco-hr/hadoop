{
  JobID jobId=profile.getJobID();
  JobContext jContext=new JobContextImpl(job,jobId);
  org.apache.hadoop.mapreduce.OutputCommitter outputCommitter=null;
  try {
    outputCommitter=createOutputCommitter(conf.getUseNewMapper(),jobId,conf);
  }
 catch (  Exception e) {
    LOG.info("Failed to createOutputCommitter",e);
    return;
  }
  try {
    TaskSplitMetaInfo[] taskSplitMetaInfos=SplitMetaInfoReader.readSplitMetaInfo(jobId,localFs,conf,systemJobDir);
    int numReduceTasks=job.getNumReduceTasks();
    if (numReduceTasks > 1 || numReduceTasks < 0) {
      numReduceTasks=1;
      job.setNumReduceTasks(1);
    }
    outputCommitter.setupJob(jContext);
    status.setSetupProgress(1.0f);
    Map<TaskAttemptID,MapOutputFile> mapOutputFiles=Collections.synchronizedMap(new HashMap<TaskAttemptID,MapOutputFile>());
    List<MapTaskRunnable> taskRunnables=getMapTaskRunnables(taskSplitMetaInfos,jobId,mapOutputFiles);
    ExecutorService mapService=createMapExecutor(taskRunnables.size());
    for (    Runnable r : taskRunnables) {
      mapService.submit(r);
    }
    try {
      mapService.shutdown();
      LOG.info("Waiting for map tasks");
      mapService.awaitTermination(Long.MAX_VALUE,TimeUnit.NANOSECONDS);
    }
 catch (    InterruptedException ie) {
      mapService.shutdownNow();
      throw ie;
    }
    LOG.info("Map task executor complete.");
    for (    MapTaskRunnable r : taskRunnables) {
      if (r.storedException != null) {
        throw new Exception(r.storedException);
      }
    }
    TaskAttemptID reduceId=new TaskAttemptID(new TaskID(jobId,TaskType.REDUCE,0),0);
    try {
      if (numReduceTasks > 0) {
        ReduceTask reduce=new ReduceTask(systemJobFile.toString(),reduceId,0,mapIds.size(),1);
        reduce.setUser(UserGroupInformation.getCurrentUser().getShortUserName());
        JobConf localConf=new JobConf(job);
        localConf.set("mapreduce.jobtracker.address","local");
        setupChildMapredLocalDirs(reduce,localConf);
        for (int i=0; i < mapIds.size(); i++) {
          if (!this.isInterrupted()) {
            TaskAttemptID mapId=mapIds.get(i);
            Path mapOut=mapOutputFiles.get(mapId).getOutputFile();
            MapOutputFile localOutputFile=new MROutputFiles();
            localOutputFile.setConf(localConf);
            Path reduceIn=localOutputFile.getInputFileForWrite(mapId.getTaskID(),localFs.getFileStatus(mapOut).getLen());
            if (!localFs.mkdirs(reduceIn.getParent())) {
              throw new IOException("Mkdirs failed to create " + reduceIn.getParent().toString());
            }
            if (!localFs.rename(mapOut,reduceIn))             throw new IOException("Couldn't rename " + mapOut);
          }
 else {
            throw new InterruptedException();
          }
        }
        if (!this.isInterrupted()) {
          reduce.setJobFile(localJobFile.toString());
          localConf.setUser(reduce.getUser());
          reduce.localizeConfiguration(localConf);
          reduce.setConf(localConf);
          reduce_tasks+=1;
          myMetrics.launchReduce(reduce.getTaskID());
          reduce.run(localConf,this);
          myMetrics.completeReduce(reduce.getTaskID());
          reduce_tasks-=1;
        }
 else {
          throw new InterruptedException();
        }
      }
    }
  finally {
      for (      MapOutputFile output : mapOutputFiles.values()) {
        output.removeAll();
      }
    }
    outputCommitter.commitJob(jContext);
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.SUCCEEDED);
    }
    JobEndNotifier.localRunnerNotification(job,status);
  }
 catch (  Throwable t) {
    try {
      outputCommitter.abortJob(jContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    }
 catch (    IOException ioe) {
      LOG.info("Error cleaning up job:" + id);
    }
    status.setCleanupProgress(1.0f);
    if (killed) {
      this.status.setRunState(JobStatus.KILLED);
    }
 else {
      this.status.setRunState(JobStatus.FAILED);
    }
    LOG.warn(id,t);
    JobEndNotifier.localRunnerNotification(job,status);
  }
 finally {
    try {
      fs.delete(systemJobFile.getParent(),true);
      localFs.delete(localJobFile,true);
      localDistributedCacheManager.close();
    }
 catch (    IOException e) {
      LOG.warn("Error cleaning up " + id + ": "+ e);
    }
  }
}
