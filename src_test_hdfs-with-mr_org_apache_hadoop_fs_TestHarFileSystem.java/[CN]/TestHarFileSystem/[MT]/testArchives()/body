{
  fs.mkdirs(inputPath);
  FSDataOutputStream out=fs.create(filea);
  out.write("a".getBytes());
  out.close();
  out=fs.create(fileb);
  out.write("b".getBytes());
  out.close();
  out=fs.create(filec);
  out.write("c".getBytes());
  out.close();
  Configuration conf=mapred.createJobConf();
  HadoopArchives har=new HadoopArchives(conf);
  String[] args=new String[3];
  args[0]="-archiveName";
  args[1]="foo.har";
  args[2]=inputPath.toString();
  int ret=ToolRunner.run(har,args);
  assertTrue(ret != 0);
  args=new String[4];
  args[0]="-archiveName";
  args[1]="/d/foo.har";
  args[2]=inputPath.toString();
  args[3]=archivePath.toString();
  ret=ToolRunner.run(har,args);
  assertTrue(ret != 0);
  args[1]="foo.har";
  args[3]=filec.toString();
  ret=ToolRunner.run(har,args);
  assertTrue(ret != 0);
  args[0]="-archiveName";
  args[1]="foo.har";
  args[2]=inputPath.toString();
  args[3]=archivePath.toString();
  ret=ToolRunner.run(har,args);
  assertTrue(ret == 0);
  ret=ToolRunner.run(har,args);
  assertTrue(ret != 0);
  Path finalPath=new Path(archivePath,"foo.har");
  Path fsPath=new Path(inputPath.toUri().getPath());
  String relative=fsPath.toString().substring(1);
  Path filePath=new Path(finalPath,relative);
  Path harPath=new Path("har://" + filePath.toUri().getPath());
  assertTrue(fs.exists(new Path(finalPath,"_index")));
  assertTrue(fs.exists(new Path(finalPath,"_masterindex")));
  assertTrue(!fs.exists(new Path(finalPath,"_logs")));
  FsShell shell=new FsShell(conf);
  args=new String[2];
  args[0]="-ls";
  args[1]=harPath.toString();
  ret=ToolRunner.run(shell,args);
  assertTrue((ret == 0));
  Path harFilea=new Path(harPath,"a");
  Path harFileb=new Path(harPath,"b");
  Path harFilec=new Path(harPath,"c");
  FileSystem harFs=harFilea.getFileSystem(conf);
  FSDataInputStream fin=harFs.open(harFilea);
  byte[] b=new byte[4];
  int readBytes=fin.read(b);
  assertTrue("Empty read.",readBytes > 0);
  fin.close();
  assertTrue("strings are equal ",(b[0] == "a".getBytes()[0]));
  fin=harFs.open(harFileb);
  readBytes=fin.read(b);
  assertTrue("Empty read.",readBytes > 0);
  fin.close();
  assertTrue("strings are equal ",(b[0] == "b".getBytes()[0]));
  fin=harFs.open(harFilec);
  readBytes=fin.read(b);
  assertTrue("Empty read.",readBytes > 0);
  fin.close();
  assertTrue("strings are equal ",(b[0] == "c".getBytes()[0]));
  Path outdir=new Path(fs.getHomeDirectory(),"mapout");
  JobConf jobconf=mapred.createJobConf();
  FileInputFormat.addInputPath(jobconf,harPath);
  jobconf.setInputFormat(TextInputFormat.class);
  jobconf.setOutputFormat(TextOutputFormat.class);
  FileOutputFormat.setOutputPath(jobconf,outdir);
  jobconf.setMapperClass(TextMapperReducer.class);
  jobconf.setMapOutputKeyClass(Text.class);
  jobconf.setMapOutputValueClass(Text.class);
  jobconf.setReducerClass(TextMapperReducer.class);
  jobconf.setNumReduceTasks(1);
  JobClient.runJob(jobconf);
  args[1]=outdir.toString();
  ret=ToolRunner.run(shell,args);
  FileStatus[] status=fs.globStatus(new Path(outdir,"part*"));
  Path reduceFile=status[0].getPath();
  FSDataInputStream reduceIn=fs.open(reduceFile);
  b=new byte[6];
  readBytes=reduceIn.read(b);
  assertTrue("Should read 6 bytes.",readBytes == 6);
  Text readTxt=new Text(b);
  assertTrue("a\nb\nc\n".equals(readTxt.toString()));
  assertTrue("number of bytes left should be -1",reduceIn.read(b) == -1);
  reduceIn.close();
}
