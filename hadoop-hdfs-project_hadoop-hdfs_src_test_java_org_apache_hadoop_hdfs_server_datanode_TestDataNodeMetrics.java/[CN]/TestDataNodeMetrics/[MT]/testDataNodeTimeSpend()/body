{
  Configuration conf=new HdfsConfiguration();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  try {
    final FileSystem fs=cluster.getFileSystem();
    List<DataNode> datanodes=cluster.getDataNodes();
    assertEquals(datanodes.size(),1);
    final DataNode datanode=datanodes.get(0);
    MetricsRecordBuilder rb=getMetrics(datanode.getMetrics().name());
    final long LONG_FILE_LEN=1024 * 1024 * 10;
    final long startWriteValue=getLongCounter("TotalWriteTime",rb);
    final long startReadValue=getLongCounter("TotalReadTime",rb);
    final AtomicInteger x=new AtomicInteger(0);
    GenericTestUtils.waitFor(new Supplier<Boolean>(){
      @Override public Boolean get(){
        x.getAndIncrement();
        try {
          DFSTestUtil.createFile(fs,new Path("/time.txt." + x.get()),LONG_FILE_LEN,(short)1,Time.monotonicNow());
          DFSTestUtil.readFile(fs,new Path("/time.txt." + x.get()));
          fs.delete(new Path("/time.txt." + x.get()),true);
        }
 catch (        IOException ioe) {
          LOG.error("Caught IOException while ingesting DN metrics",ioe);
          return false;
        }
        MetricsRecordBuilder rbNew=getMetrics(datanode.getMetrics().name());
        final long endWriteValue=getLongCounter("TotalWriteTime",rbNew);
        final long endReadValue=getLongCounter("TotalReadTime",rbNew);
        return endWriteValue > startWriteValue && endReadValue > startReadValue;
      }
    }
,30,60000);
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}
