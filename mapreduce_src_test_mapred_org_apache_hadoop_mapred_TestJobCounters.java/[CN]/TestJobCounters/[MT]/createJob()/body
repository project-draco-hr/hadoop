{
  final Configuration conf=new Configuration();
  final Job baseJob=Job.getInstance(new Cluster(conf),conf);
  baseJob.setOutputKeyClass(Text.class);
  baseJob.setOutputValueClass(IntWritable.class);
  baseJob.setMapperClass(NewMapTokenizer.class);
  baseJob.setCombinerClass(NewSummer.class);
  baseJob.setReducerClass(NewSummer.class);
  baseJob.setNumReduceTasks(1);
  baseJob.getConfiguration().setInt(JobContext.IO_SORT_MB,1);
  baseJob.getConfiguration().set(JobContext.MAP_SORT_SPILL_PERCENT,"0.50");
  baseJob.getConfiguration().setInt(JobContext.MAP_COMBINE_MIN_SPILLS,3);
  org.apache.hadoop.mapreduce.lib.input.FileInputFormat.setMinInputSplitSize(baseJob,Long.MAX_VALUE);
  return baseJob;
}
