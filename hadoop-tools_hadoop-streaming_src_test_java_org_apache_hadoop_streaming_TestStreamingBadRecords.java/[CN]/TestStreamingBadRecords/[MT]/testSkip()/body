{
  JobConf clusterConf=createJobConf();
  createInput();
  int attSkip=0;
  SkipBadRecords.setAttemptsToStartSkipping(clusterConf,attSkip);
  int mapperAttempts=attSkip + 1 + MAPPER_BAD_RECORDS.size();
  int reducerAttempts=attSkip + 1 + REDUCER_BAD_RECORDS.size();
  String[] args=new String[]{"-input",(new Path(getInputDir(),"text.txt")).toString(),"-output",getOutputDir().toString(),"-mapper",badMapper,"-reducer",badReducer,"-verbose","-inputformat","org.apache.hadoop.mapred.KeyValueTextInputFormat","-jobconf","mapreduce.task.skip.start.attempts=" + attSkip,"-jobconf","mapreduce.job.skip.outdir=none","-jobconf","mapreduce.map.maxattempts=" + mapperAttempts,"-jobconf","mapreduce.reduce.maxattempts=" + reducerAttempts,"-jobconf","mapreduce.map.skip.maxrecords=" + Long.MAX_VALUE,"-jobconf","mapreduce.reduce.skip.maxgroups=" + Long.MAX_VALUE,"-jobconf","mapreduce.job.maps=1","-jobconf","mapreduce.job.reduces=1","-jobconf","fs.default.name=" + clusterConf.get("fs.default.name"),"-jobconf","mapreduce.jobtracker.address=" + clusterConf.get(JTConfig.JT_IPC_ADDRESS),"-jobconf","mapreduce.jobtracker.http.address=" + clusterConf.get(JTConfig.JT_HTTP_ADDRESS),"-jobconf","mapreduce.task.files.preserve.failedtasks=true","-jobconf","stream.tmpdir=" + System.getProperty("test.build.data","/tmp"),"-jobconf","mapred.jar=" + TestStreaming.STREAMING_JAR,"-jobconf","mapreduce.framework.name=yarn"};
  StreamJob job=new StreamJob(args,false);
  job.go();
  validateOutput(job.running_,false);
  assertTrue(SkipBadRecords.getSkipOutputPath(job.jobConf_) == null);
}
