{
  createInput();
  JobConf clusterConf=createJobConf();
  String[] args=new String[]{"-input",(new Path(getInputDir(),"text.txt")).toString(),"-output",getOutputDir().toString(),"-mapper",badMapper,"-reducer",badReducer,"-verbose","-inputformat","org.apache.hadoop.mapred.KeyValueTextInputFormat","-jobconf","mapreduce.task.skip.start.attempts=1","-jobconf","mapreduce.map.maxattempts=20","-jobconf","mapreduce.reduce.maxattempts=15","-jobconf","mapreduce.map.skip.maxrecords=1","-jobconf","mapreduce.reduce.skip.maxgroups=1","-jobconf","mapreduce.job.maps=1","-jobconf","mapreduce.job.reduces=1","-jobconf","fs.default.name=" + clusterConf.get("fs.default.name"),"-jobconf","mapreduce.jobtracker.address=" + clusterConf.get(JTConfig.JT_IPC_ADDRESS),"-jobconf","mapreduce.jobtracker.http.address=" + clusterConf.get(JTConfig.JT_HTTP_ADDRESS),"-jobconf","mapreduce.task.files.preserve.failedtasks=true","-jobconf","stream.tmpdir=" + System.getProperty("test.build.data","/tmp"),"-jobconf","mapred.jar=" + TestStreaming.STREAMING_JAR,"-jobconf","mapreduce.framework.name=yarn"};
  StreamJob job=new StreamJob(args,false);
  job.go();
  validateOutput(job.running_,true);
  assertTrue(SkipBadRecords.getSkipOutputPath(job.jobConf_) != null);
}
