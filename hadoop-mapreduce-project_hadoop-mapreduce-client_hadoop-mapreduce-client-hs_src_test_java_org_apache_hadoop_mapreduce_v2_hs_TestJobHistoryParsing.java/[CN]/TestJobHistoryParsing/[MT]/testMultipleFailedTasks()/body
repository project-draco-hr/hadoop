{
  JobHistoryParser parser=new JobHistoryParser(Mockito.mock(FSDataInputStream.class));
  EventReader reader=Mockito.mock(EventReader.class);
  final AtomicInteger numEventsRead=new AtomicInteger(0);
  final org.apache.hadoop.mapreduce.TaskType taskType=org.apache.hadoop.mapreduce.TaskType.MAP;
  final TaskID[] tids=new TaskID[2];
  final JobID jid=new JobID("1",1);
  tids[0]=new TaskID(jid,taskType,0);
  tids[1]=new TaskID(jid,taskType,1);
  Mockito.when(reader.getNextEvent()).thenAnswer(new Answer<HistoryEvent>(){
    public HistoryEvent answer(    InvocationOnMock invocation) throws IOException {
      int eventId=numEventsRead.getAndIncrement();
      TaskID tid=tids[eventId & 0x1];
      if (eventId < 2) {
        return new TaskStartedEvent(tid,0,taskType,"");
      }
      if (eventId < 4) {
        TaskFailedEvent tfe=new TaskFailedEvent(tid,0,taskType,"failed","FAILED",null,new Counters());
        tfe.setDatum(tfe.getDatum());
        return tfe;
      }
      if (eventId < 5) {
        JobUnsuccessfulCompletionEvent juce=new JobUnsuccessfulCompletionEvent(jid,100L,2,0,"JOB_FAILED",Collections.singletonList("Task failed: " + tids[0].toString()));
        return juce;
      }
      return null;
    }
  }
);
  JobInfo info=parser.parse(reader);
  assertTrue("Task 0 not implicated",info.getErrorInfo().contains(tids[0].toString()));
}
