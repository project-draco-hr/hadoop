{
  conf.set(DFS_DATANODE_HOST_NAME_KEY,"127.0.0.1");
  int curDatanodesNum=dataNodes.size();
  if (conf.get(DFS_BLOCKREPORT_INITIAL_DELAY_KEY) == null) {
    conf.setLong(DFS_BLOCKREPORT_INITIAL_DELAY_KEY,0);
  }
  if (racks != null && numDataNodes > racks.length) {
    throw new IllegalArgumentException("The length of racks [" + racks.length + "] is less than the number of datanodes ["+ numDataNodes+ "].");
  }
  if (hosts != null && numDataNodes > hosts.length) {
    throw new IllegalArgumentException("The length of hosts [" + hosts.length + "] is less than the number of datanodes ["+ numDataNodes+ "].");
  }
  if (racks != null && hosts == null) {
    hosts=new String[numDataNodes];
    for (int i=curDatanodesNum; i < curDatanodesNum + numDataNodes; i++) {
      hosts[i - curDatanodesNum]="host" + i + ".foo.com";
    }
  }
  if (simulatedCapacities != null && numDataNodes > simulatedCapacities.length) {
    throw new IllegalArgumentException("The length of simulatedCapacities [" + simulatedCapacities.length + "] is less than the number of datanodes ["+ numDataNodes+ "].");
  }
  String[] dnArgs=(operation == null || operation != StartupOption.ROLLBACK) ? null : new String[]{operation.getName()};
  for (int i=curDatanodesNum; i < curDatanodesNum + numDataNodes; i++) {
    Configuration dnConf=new HdfsConfiguration(conf);
    setupDatanodeAddress(dnConf,setupHostsFile,checkDataNodeAddrConfig);
    if (manageDfsDirs) {
      File dir1=getInstanceStorageDir(i,0);
      File dir2=getInstanceStorageDir(i,1);
      dir1.mkdirs();
      dir2.mkdirs();
      if (!dir1.isDirectory() || !dir2.isDirectory()) {
        throw new IOException("Mkdirs failed to create directory for DataNode " + i + ": "+ dir1+ " or "+ dir2);
      }
      String dirs=fileAsURI(dir1) + "," + fileAsURI(dir2);
      dnConf.set(DFS_DATANODE_DATA_DIR_KEY,dirs);
      conf.set(DFS_DATANODE_DATA_DIR_KEY,dirs);
    }
    if (simulatedCapacities != null) {
      dnConf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED,true);
      dnConf.setLong(SimulatedFSDataset.CONFIG_PROPERTY_CAPACITY,simulatedCapacities[i - curDatanodesNum]);
    }
    LOG.info("Starting DataNode " + i + " with "+ DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY+ ": "+ dnConf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));
    if (hosts != null) {
      dnConf.set(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY,hosts[i - curDatanodesNum]);
      LOG.info("Starting DataNode " + i + " with hostname set to: "+ dnConf.get(DFSConfigKeys.DFS_DATANODE_HOST_NAME_KEY));
    }
    if (racks != null) {
      String name=hosts[i - curDatanodesNum];
      LOG.info("Adding node with hostname : " + name + " to rack "+ racks[i - curDatanodesNum]);
      StaticMapping.addNodeToRack(name,racks[i - curDatanodesNum]);
    }
    Configuration newconf=new HdfsConfiguration(dnConf);
    if (hosts != null) {
      NetUtils.addStaticResolution(hosts[i - curDatanodesNum],"localhost");
    }
    DataNode dn=DataNode.instantiateDataNode(dnArgs,dnConf);
    if (dn == null)     throw new IOException("Cannot start DataNode in " + dnConf.get(DFS_DATANODE_DATA_DIR_KEY));
    String ipAddr=dn.getSelfAddr().getAddress().getHostAddress();
    if (racks != null) {
      int port=dn.getSelfAddr().getPort();
      LOG.info("Adding node with IP:port : " + ipAddr + ":"+ port+ " to rack "+ racks[i - curDatanodesNum]);
      StaticMapping.addNodeToRack(ipAddr + ":" + port,racks[i - curDatanodesNum]);
    }
    dn.runDatanodeDaemon();
    dataNodes.add(new DataNodeProperties(dn,newconf,dnArgs));
  }
  curDatanodesNum+=numDataNodes;
  this.numDataNodes+=numDataNodes;
  waitActive();
}
