{
  this.conf=conf;
  base_dir=new File(determineDfsBaseDir());
  data_dir=new File(base_dir,"data");
  this.waitSafeMode=waitSafeMode;
  String rpcEngineName=System.getProperty("hdfs.rpc.engine");
  if (rpcEngineName != null && !"".equals(rpcEngineName)) {
    LOG.info("HDFS using RPCEngine: " + rpcEngineName);
    try {
      Class<?> rpcEngine=conf.getClassByName(rpcEngineName);
      setRpcEngine(conf,NamenodeProtocols.class,rpcEngine);
      setRpcEngine(conf,ClientNamenodeWireProtocol.class,rpcEngine);
      setRpcEngine(conf,ClientDatanodeProtocolPB.class,rpcEngine);
      setRpcEngine(conf,NamenodeProtocolPB.class,rpcEngine);
      setRpcEngine(conf,ClientProtocol.class,rpcEngine);
      setRpcEngine(conf,DatanodeProtocol.class,rpcEngine);
      setRpcEngine(conf,RefreshAuthorizationPolicyProtocol.class,rpcEngine);
      setRpcEngine(conf,RefreshUserMappingsProtocol.class,rpcEngine);
      setRpcEngine(conf,GetUserMappingsProtocol.class,rpcEngine);
    }
 catch (    ClassNotFoundException e) {
      throw new RuntimeException(e);
    }
    conf.setBoolean(HADOOP_SECURITY_AUTHORIZATION,false);
  }
  int replication=conf.getInt(DFS_REPLICATION_KEY,3);
  conf.setInt(DFS_REPLICATION_KEY,Math.min(replication,numDataNodes));
  conf.setInt(DFS_NAMENODE_SAFEMODE_EXTENSION_KEY,0);
  conf.setInt(DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY,3);
  conf.setClass(NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,StaticMapping.class,DNSToSwitchMapping.class);
  federation=nnTopology.isFederated();
  createNameNodesAndSetConf(nnTopology,manageNameDfsDirs,format,operation,clusterId,conf);
  if (format) {
    if (data_dir.exists() && !FileUtil.fullyDelete(data_dir)) {
      throw new IOException("Cannot remove data directory: " + data_dir);
    }
  }
  startDataNodes(conf,numDataNodes,manageDataDfsDirs,operation,racks,hosts,simulatedCapacities,setupHostsFile);
  waitClusterUp();
  ProxyUsers.refreshSuperUserGroupsConfiguration(conf);
}
