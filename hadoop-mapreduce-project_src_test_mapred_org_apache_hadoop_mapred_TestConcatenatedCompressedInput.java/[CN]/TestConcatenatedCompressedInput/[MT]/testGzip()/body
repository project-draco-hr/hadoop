{
  JobConf jobConf=new JobConf(defaultConf);
  CompressionCodec gzip=new GzipCodec();
  ReflectionUtils.setConf(gzip,jobConf);
  localFs.delete(workDir,true);
  if (org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class == gzip.getDecompressorType()) {
    System.out.println(COLOR_BR_RED + "testGzip() using native-zlib Decompressor (" + gzip.getDecompressorType()+ ")"+ COLOR_NORMAL);
  }
 else {
    LOG.warn("testGzip() skipped:  native (C/C++) libs not loaded");
    return;
  }
  final String fn="concat" + gzip.getDefaultExtension();
  Path fnLocal=new Path(System.getProperty("test.concat.data","/tmp"),fn);
  Path fnHDFS=new Path(workDir,fn);
  localFs.copyFromLocalFile(fnLocal,fnHDFS);
  writeFile(localFs,new Path(workDir,"part2.txt.gz"),gzip,"this is a test\nof gzip\n");
  FileInputFormat.setInputPaths(jobConf,workDir);
  TextInputFormat format=new TextInputFormat();
  format.configure(jobConf);
  InputSplit[] splits=format.getSplits(jobConf,100);
  assertEquals("compressed splits == 2",2,splits.length);
  FileSplit tmp=(FileSplit)splits[0];
  if (tmp.getPath().getName().equals("part2.txt.gz")) {
    splits[0]=splits[1];
    splits[1]=tmp;
  }
  List<Text> results=readSplit(format,splits[0],jobConf);
  assertEquals("splits[0] num lines",6,results.size());
  assertEquals("splits[0][5]","member #3",results.get(5).toString());
  results=readSplit(format,splits[1],jobConf);
  assertEquals("splits[1] num lines",2,results.size());
  assertEquals("splits[1][0]","this is a test",results.get(0).toString());
  assertEquals("splits[1][1]","of gzip",results.get(1).toString());
}
