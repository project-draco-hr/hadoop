{
  super.createTypeMembers(newType,imports,monitor);
  imports.addImport("org.apache.hadoop.fs.Path");
  imports.addImport("org.apache.hadoop.io.Text");
  imports.addImport("org.apache.hadoop.io.IntWritable");
  imports.addImport("org.apache.hadoop.mapred.JobClient");
  imports.addImport("org.apache.hadoop.mapred.JobConf");
  imports.addImport("org.apache.hadoop.mapred.Reducer");
  imports.addImport("org.apache.hadoop.mapred.Mapper");
  getContainer().getShell().getDisplay().syncExec(new Runnable(){
    public void run(){
      String method="public static void main(String[] args) {\n JobClient client = new JobClient();";
      method+="JobConf conf = new JobConf(" + newType.getFullyQualifiedName() + ".class);\n\n";
      method+="// TODO: specify output types\nconf.setOutputKeyClass(Text.class);\nconf.setOutputValueClass(IntWritable.class);\n\n";
      method+="// TODO: specify input and output DIRECTORIES (not files)\nconf.setInputPath(new Path(\"src\"));\nconf.setOutputPath(new Path(\"out\"));\n\n";
      if (mapperText.getText().length() > 0) {
        method+="conf.setMapperClass(" + mapperText.getText() + ".class);\n\n";
      }
 else {
        method+="// TODO: specify a mapper\nconf.setMapperClass(org.apache.hadoop.mapred.lib.IdentityMapper.class);\n\n";
      }
      if (reducerText.getText().length() > 0) {
        method+="conf.setReducerClass(" + reducerText.getText() + ".class);\n\n";
      }
 else {
        method+="// TODO: specify a reducer\nconf.setReducerClass(org.apache.hadoop.mapred.lib.IdentityReducer.class);\n\n";
      }
      method+="client.setConf(conf);\n";
      method+="try {\n\tJobClient.runJob(conf);\n} catch (Exception e) {\n" + "\te.printStackTrace();\n}\n";
      method+="}\n";
      try {
        newType.createMethod(method,null,false,monitor);
      }
 catch (      JavaModelException e) {
        e.printStackTrace();
      }
    }
  }
);
}
