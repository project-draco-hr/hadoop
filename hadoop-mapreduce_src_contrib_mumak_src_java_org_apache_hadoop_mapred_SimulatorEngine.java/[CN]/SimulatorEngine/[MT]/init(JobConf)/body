{
  FileSystem lfs=FileSystem.getLocal(getConf());
  Path logPath=new Path(System.getProperty("hadoop.log.dir")).makeQualified(lfs);
  jobConf.set("mapred.system.dir",logPath.toString());
  jobConf.set("hadoop.job.history.location",(new Path(logPath,"history").toString()));
  long now=getTimeProperty(jobConf,"mumak.start.time",System.currentTimeMillis());
  jt=SimulatorJobTracker.startTracker(jobConf,now,this);
  jt.offerService();
  masterRandomSeed=jobConf.getLong("mumak.random.seed",System.nanoTime());
  int maxMaps=getConf().getInt("mapred.tasktracker.map.tasks.maximum",SimulatorTaskTracker.DEFAULT_MAP_SLOTS);
  int maxReduces=getConf().getInt("mapred.tasktracker.reduce.tasks.maximum",SimulatorTaskTracker.DEFAULT_REDUCE_SLOTS);
  MachineNode defaultNode=new MachineNode.Builder("default",2).setMapSlots(maxMaps).setReduceSlots(maxReduces).build();
  LoggedNetworkTopology topology=new ClusterTopologyReader(new Path(topologyFile),jobConf).get();
  setStaticMapping(topology);
  if (getConf().getBoolean("mumak.topology.filter-numeric-ips",true)) {
    removeIpHosts(topology);
  }
  ZombieCluster cluster=new ZombieCluster(topology,defaultNode);
  long firstJobStartTime=startTaskTrackers(cluster,jobConf,now);
  long subRandomSeed=RandomSeedGenerator.getSeed("forSimulatorJobStoryProducer",masterRandomSeed);
  JobStoryProducer jobStoryProducer=new SimulatorJobStoryProducer(new Path(traceFile),cluster,firstJobStartTime,jobConf,subRandomSeed);
  final SimulatorJobSubmissionPolicy submissionPolicy=SimulatorJobSubmissionPolicy.getPolicy(jobConf);
  jc=new SimulatorJobClient(jt,jobStoryProducer,submissionPolicy);
  queue.addAll(jc.init(firstJobStartTime));
  if (jobConf.get("mapred.jobtracker.taskScheduler").equals(CapacityTaskScheduler.class.getName())) {
    LOG.info("CapacityScheduler used: starting simulatorThreads");
    startSimulatorThreadsCapSched(now);
  }
  terminateTime=getTimeProperty(jobConf,"mumak.terminate.time",Long.MAX_VALUE);
}
