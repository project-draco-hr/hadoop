{
  MiniDFSCluster cluster=null;
  String testFile="/replication-test-file";
  Path testPath=new Path(testFile);
  byte buffer[]=new byte[1024];
  for (int i=0; i < buffer.length; i++) {
    buffer[i]='1';
  }
  try {
    Configuration conf=new HdfsConfiguration();
    conf.set(DFSConfigKeys.DFS_REPLICATION_KEY,Integer.toString(numDataNodes));
    conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,checksumSize);
    SimulatedFSDataset.setFactory(conf);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    cluster.waitActive();
    String bpid=cluster.getNamesystem().getBlockPoolId();
    DFSClient dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
    DFSTestUtil.createFile(cluster.getFileSystem(),testPath,filesize,filesize,blockSize,(short)numDataNodes,0L);
    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,20);
    List<Map<String,BlockListAsLongs>> blocksList=cluster.getAllBlockReports(bpid);
    cluster.shutdown();
    cluster=null;
    LOG.info("Restarting minicluster");
    conf=new HdfsConfiguration();
    SimulatedFSDataset.setFactory(conf);
    conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes * 2).format(false).build();
    cluster.waitActive();
    Set<Block> uniqueBlocks=new HashSet<Block>();
    for (    Map<String,BlockListAsLongs> map : blocksList) {
      for (      BlockListAsLongs blockList : map.values()) {
        for (        Block b : blockList) {
          uniqueBlocks.add(new Block(b));
        }
      }
    }
    LOG.info("Inserting " + uniqueBlocks.size() + " blocks");
    cluster.injectBlocks(0,uniqueBlocks);
    dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
    waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,-1);
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}
