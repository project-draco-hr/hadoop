{
  final RawKeyValueIterator rawIter=rIter;
  rIter=new RawKeyValueIterator(){
    public void close() throws IOException {
      rawIter.close();
    }
    public DataInputBuffer getKey() throws IOException {
      return rawIter.getKey();
    }
    public Progress getProgress(){
      return rawIter.getProgress();
    }
    public DataInputBuffer getValue() throws IOException {
      return rawIter.getValue();
    }
    public boolean next() throws IOException {
      boolean ret=rawIter.next();
      reporter.setProgress(rawIter.getProgress().getProgress());
      return ret;
    }
  }
;
  org.apache.hadoop.mapreduce.TaskAttemptContext taskContext=new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job,getTaskID(),reporter);
  org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE> reducer=(org.apache.hadoop.mapreduce.Reducer<INKEY,INVALUE,OUTKEY,OUTVALUE>)ReflectionUtils.newInstance(taskContext.getReducerClass(),job);
  org.apache.hadoop.mapreduce.RecordWriter<OUTKEY,OUTVALUE> trackedRW=new NewTrackingRecordWriter<OUTKEY,OUTVALUE>(this,taskContext);
  job.setBoolean("mapred.skip.on",isSkipping());
  job.setBoolean(JobContext.SKIP_RECORDS,isSkipping());
  org.apache.hadoop.mapreduce.Reducer.Context reducerContext=createReduceContext(reducer,job,getTaskID(),rIter,reduceInputKeyCounter,reduceInputValueCounter,trackedRW,committer,reporter,comparator,keyClass,valueClass);
  try {
    reducer.run(reducerContext);
  }
  finally {
    trackedRW.close(reducerContext);
  }
}
