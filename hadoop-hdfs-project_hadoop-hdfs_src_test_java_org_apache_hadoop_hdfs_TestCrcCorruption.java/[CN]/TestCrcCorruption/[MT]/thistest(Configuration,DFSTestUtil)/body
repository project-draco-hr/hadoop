{
  MiniDFSCluster cluster=null;
  int numDataNodes=2;
  short replFactor=2;
  Random random=new Random();
  conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE,10);
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    util.createFiles(fs,"/srcdat",replFactor);
    util.waitReplication(fs,"/srcdat",(short)2);
    File storageDir=cluster.getInstanceStorageDir(0,1);
    String bpid=cluster.getNamesystem().getBlockPoolId();
    File data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);
    assertTrue("data directory does not exist",data_dir.exists());
    File[] blocks=data_dir.listFiles();
    assertTrue("Blocks do not exist in data-dir",(blocks != null) && (blocks.length > 0));
    int num=0;
    for (int idx=0; idx < blocks.length; idx++) {
      if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {
        num++;
        if (num % 3 == 0) {
          System.out.println("Deliberately removing file " + blocks[idx].getName());
          assertTrue("Cannot remove file.",blocks[idx].delete());
        }
 else         if (num % 3 == 1) {
          RandomAccessFile file=new RandomAccessFile(blocks[idx],"rw");
          FileChannel channel=file.getChannel();
          int newsize=random.nextInt((int)channel.size() / 2);
          System.out.println("Deliberately truncating file " + blocks[idx].getName() + " to size "+ newsize+ " bytes.");
          channel.truncate(newsize);
          file.close();
        }
 else {
          RandomAccessFile file=new RandomAccessFile(blocks[idx],"rw");
          FileChannel channel=file.getChannel();
          long position=0;
          if (num != 2) {
            position=(long)random.nextInt((int)channel.size());
          }
          int length=random.nextInt((int)(channel.size() - position + 1));
          byte[] buffer=new byte[length];
          random.nextBytes(buffer);
          channel.write(ByteBuffer.wrap(buffer),position);
          System.out.println("Deliberately corrupting file " + blocks[idx].getName() + " at offset "+ position+ " length "+ length);
          file.close();
        }
      }
    }
    storageDir=cluster.getInstanceStorageDir(0,1);
    data_dir=MiniDFSCluster.getFinalizedDir(storageDir,bpid);
    assertTrue("data directory does not exist",data_dir.exists());
    blocks=data_dir.listFiles();
    assertTrue("Blocks do not exist in data-dir",(blocks != null) && (blocks.length > 0));
    int count=0;
    File previous=null;
    for (int idx=0; idx < blocks.length; idx++) {
      if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {
        count++;
        if (count % 2 == 0) {
          System.out.println("Deliberately insertimg bad crc into files " + blocks[idx].getName() + " "+ previous.getName());
          assertTrue("Cannot remove file.",blocks[idx].delete());
          assertTrue("Cannot corrupt meta file.",previous.renameTo(blocks[idx]));
          assertTrue("Cannot recreate empty meta file.",previous.createNewFile());
          previous=null;
        }
 else {
          previous=blocks[idx];
        }
      }
    }
    assertTrue("Corrupted replicas not handled properly.",util.checkFiles(fs,"/srcdat"));
    System.out.println("All File still have a valid replica");
    util.setReplication(fs,"/srcdat",(short)1);
    System.out.println("The excess-corrupted-replica test is disabled " + " pending HADOOP-1557");
    util.cleanup(fs,"/srcdat");
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}
