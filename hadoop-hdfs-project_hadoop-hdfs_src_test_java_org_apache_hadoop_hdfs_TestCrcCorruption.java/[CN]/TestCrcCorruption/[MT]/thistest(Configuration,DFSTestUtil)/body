{
  MiniDFSCluster cluster=null;
  int numDataNodes=2;
  short replFactor=2;
  Random random=new Random();
  conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,10);
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    util.createFiles(fs,"/srcdat",replFactor);
    util.waitReplication(fs,"/srcdat",(short)2);
    final int dnIdx=0;
    final DataNode dn=cluster.getDataNodes().get(dnIdx);
    final String bpid=cluster.getNamesystem().getBlockPoolId();
    List<FinalizedReplica> replicas=dn.getFSDataset().getFinalizedBlocks(bpid);
    assertTrue("Replicas do not exist",!replicas.isEmpty());
    for (int idx=0; idx < replicas.size(); idx++) {
      FinalizedReplica replica=replicas.get(idx);
      ExtendedBlock eb=new ExtendedBlock(bpid,replica);
      if (idx % 3 == 0) {
        LOG.info("Deliberately removing meta for block " + eb);
        cluster.deleteMeta(dnIdx,eb);
      }
 else       if (idx % 3 == 1) {
        final int newSize=2;
        LOG.info("Deliberately truncating meta file for block " + eb + " to size "+ newSize+ " bytes.");
        cluster.truncateMeta(dnIdx,eb,newSize);
      }
 else {
        cluster.corruptMeta(dnIdx,eb);
      }
    }
    assertTrue("Corrupted replicas not handled properly.",util.checkFiles(fs,"/srcdat"));
    LOG.info("All File still have a valid replica");
    util.setReplication(fs,"/srcdat",(short)1);
    LOG.info("The excess-corrupted-replica test is disabled " + " pending HADOOP-1557");
    util.cleanup(fs,"/srcdat");
  }
  finally {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}
