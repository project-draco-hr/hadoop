{
  fs.delete(archivePath,true);
  Configuration conf=mapred.createJobConf();
  HadoopArchives har=new HadoopArchives(conf);
  String[] args=new String[8];
  args[0]="-Dhar.block.size=512";
  args[1]="-Dhar.partfile.size=1";
  args[2]="-archiveName";
  args[3]="foo bar.har";
  args[4]="-p";
  args[5]=fs.getHomeDirectory().toString();
  args[6]="test";
  args[7]=archivePath.toString();
  int ret=ToolRunner.run(har,args);
  assertTrue("failed test",ret == 0);
  Path finalPath=new Path(archivePath,"foo bar.har");
  Path fsPath=new Path(inputPath.toUri().getPath());
  Path filePath=new Path(finalPath,"test");
  Path filea=new Path(filePath,"a");
  Path harPath=new Path("har://" + filea.toUri().getPath());
  FileSystem harFs=harPath.getFileSystem(conf);
  FileStatus[] statuses=harFs.listStatus(filePath);
  for (  FileStatus status : statuses) {
    BlockLocation[] locations=harFs.getFileBlockLocations(status,0,status.getLen());
    long lastOffset=0;
    assertEquals("Only one block location expected for files this small",1,locations.length);
    assertEquals("Block location should start at offset 0",0,locations[0].getOffset());
  }
}
