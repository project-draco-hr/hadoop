{
  int dataLen=decodingState.decodeLength;
  CoderUtil.resetOutputBuffers(decodingState.outputs,dataLen);
  int[] erasedOrNotToReadIndexes=CoderUtil.getNullIndexes(decodingState.inputs);
  ByteBuffer[] directBuffers=new ByteBuffer[getNumParityUnits()];
  ByteBuffer[] adjustedDirectBufferOutputsParameter=new ByteBuffer[getNumParityUnits()];
  for (int outputIdx=0, i=0; i < decodingState.erasedIndexes.length; i++) {
    boolean found=false;
    for (int j=0; j < erasedOrNotToReadIndexes.length; j++) {
      if (decodingState.erasedIndexes[i] == erasedOrNotToReadIndexes[j]) {
        found=true;
        adjustedDirectBufferOutputsParameter[j]=CoderUtil.resetBuffer(decodingState.outputs[outputIdx++],dataLen);
      }
    }
    if (!found) {
      throw new HadoopIllegalArgumentException("Inputs not fully corresponding to erasedIndexes in null places");
    }
  }
  for (int bufferIdx=0, i=0; i < erasedOrNotToReadIndexes.length; i++) {
    if (adjustedDirectBufferOutputsParameter[i] == null) {
      ByteBuffer buffer=checkGetDirectBuffer(directBuffers,bufferIdx,dataLen);
      buffer.position(0);
      buffer.limit(dataLen);
      adjustedDirectBufferOutputsParameter[i]=CoderUtil.resetBuffer(buffer,dataLen);
      bufferIdx++;
    }
  }
  doDecodeImpl(decodingState.inputs,erasedOrNotToReadIndexes,adjustedDirectBufferOutputsParameter);
}
