{
  StopWatch sw=new StopWatch().start();
  FileStatus[] files=listStatus(job);
  job.setLong(NUM_INPUT_FILES,files.length);
  long totalSize=0;
  for (  FileStatus file : files) {
    if (file.isDirectory()) {
      throw new IOException("Not a file: " + file.getPath());
    }
    totalSize+=file.getLen();
  }
  long goalSize=totalSize / (numSplits == 0 ? 1 : numSplits);
  long minSize=Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE,1),minSplitSize);
  ArrayList<FileSplit> splits=new ArrayList<FileSplit>(numSplits);
  NetworkTopology clusterMap=new NetworkTopology();
  for (  FileStatus file : files) {
    Path path=file.getPath();
    long length=file.getLen();
    if (length != 0) {
      FileSystem fs=path.getFileSystem(job);
      BlockLocation[] blkLocations;
      if (file instanceof LocatedFileStatus) {
        blkLocations=((LocatedFileStatus)file).getBlockLocations();
      }
 else {
        blkLocations=fs.getFileBlockLocations(file,0,length);
      }
      if (isSplitable(fs,path)) {
        long blockSize=file.getBlockSize();
        long splitSize=computeSplitSize(goalSize,minSize,blockSize);
        long bytesRemaining=length;
        while (((double)bytesRemaining) / splitSize > SPLIT_SLOP) {
          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,splitSize,clusterMap);
          splits.add(makeSplit(path,length - bytesRemaining,splitSize,splitHosts[0],splitHosts[1]));
          bytesRemaining-=splitSize;
        }
        if (bytesRemaining != 0) {
          String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,length - bytesRemaining,bytesRemaining,clusterMap);
          splits.add(makeSplit(path,length - bytesRemaining,bytesRemaining,splitHosts[0],splitHosts[1]));
        }
      }
 else {
        if (LOG.isDebugEnabled()) {
          if (length > Math.min(file.getBlockSize(),minSize)) {
            LOG.debug("File is not splittable so no parallelization " + "is possible: " + file.getPath());
          }
        }
        String[][] splitHosts=getSplitHostsAndCachedHosts(blkLocations,0,length,clusterMap);
        splits.add(makeSplit(path,0,length,splitHosts[0],splitHosts[1]));
      }
    }
 else {
      splits.add(makeSplit(path,0,length,new String[0]));
    }
  }
  sw.stop();
  if (LOG.isDebugEnabled()) {
    LOG.debug("Total # of splits generated by getSplits: " + splits.size() + ", TimeTaken: "+ sw.now(TimeUnit.MILLISECONDS));
  }
  return splits.toArray(new FileSplit[splits.size()]);
}
