{
  Path[] dirs=getInputPaths(job);
  if (dirs.length == 0) {
    throw new IOException("No input paths specified in job");
  }
  TokenCache.obtainTokensForNamenodes(job.getCredentials(),dirs,job);
  boolean recursive=job.getBoolean(INPUT_DIR_RECURSIVE,false);
  List<PathFilter> filters=new ArrayList<PathFilter>();
  filters.add(hiddenFileFilter);
  PathFilter jobFilter=getInputPathFilter(job);
  if (jobFilter != null) {
    filters.add(jobFilter);
  }
  PathFilter inputFilter=new MultiPathFilter(filters);
  FileStatus[] result;
  int numThreads=job.getInt(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.LIST_STATUS_NUM_THREADS,org.apache.hadoop.mapreduce.lib.input.FileInputFormat.DEFAULT_LIST_STATUS_NUM_THREADS);
  StopWatch sw=new StopWatch().start();
  if (numThreads == 1) {
    List<FileStatus> locatedFiles=singleThreadedListStatus(job,dirs,inputFilter,recursive);
    result=locatedFiles.toArray(new FileStatus[locatedFiles.size()]);
  }
 else {
    Iterable<FileStatus> locatedFiles=null;
    try {
      LocatedFileStatusFetcher locatedFileStatusFetcher=new LocatedFileStatusFetcher(job,dirs,recursive,inputFilter,false);
      locatedFiles=locatedFileStatusFetcher.getFileStatuses();
    }
 catch (    InterruptedException e) {
      throw new IOException("Interrupted while getting file statuses");
    }
    result=Iterables.toArray(locatedFiles,FileStatus.class);
  }
  sw.stop();
  if (LOG.isDebugEnabled()) {
    LOG.debug("Time taken to get FileStatuses: " + sw.now(TimeUnit.MILLISECONDS));
  }
  LOG.info("Total input paths to process : " + result.length);
  return result;
}
