{
  HdfsConfiguration conf=initZeroCopyTest();
  MiniDFSCluster cluster=null;
  final Path TEST_PATH=new Path("/a");
  FSDataInputStream fsIn=null;
  final int TEST_FILE_LENGTH=3 * BLOCK_SIZE;
  FileSystem fs=null;
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,TEST_PATH,TEST_FILE_LENGTH,(short)1,7567L);
    try {
      DFSTestUtil.waitReplication(fs,TEST_PATH,(short)1);
    }
 catch (    InterruptedException e) {
      Assert.fail("unexpected InterruptedException during " + "waitReplication: " + e);
    }
catch (    TimeoutException e) {
      Assert.fail("unexpected TimeoutException during " + "waitReplication: " + e);
    }
    fsIn=fs.open(TEST_PATH);
    byte original[]=new byte[TEST_FILE_LENGTH];
    IOUtils.readFully(fsIn,original,0,TEST_FILE_LENGTH);
    fsIn.close();
    fsIn=fs.open(TEST_PATH);
    HdfsDataInputStream dfsIn=(HdfsDataInputStream)fsIn;
    ByteBuffer result;
    try {
      result=dfsIn.read(null,BLOCK_SIZE + 1,EnumSet.noneOf(ReadOption.class));
      Assert.fail("expected UnsupportedOperationException");
    }
 catch (    UnsupportedOperationException e) {
    }
    result=dfsIn.read(null,BLOCK_SIZE,EnumSet.of(ReadOption.SKIP_CHECKSUMS));
    Assert.assertEquals(BLOCK_SIZE,result.remaining());
    Assert.assertEquals(BLOCK_SIZE,dfsIn.getReadStatistics().getTotalBytesRead());
    Assert.assertEquals(BLOCK_SIZE,dfsIn.getReadStatistics().getTotalZeroCopyBytesRead());
    Assert.assertArrayEquals(Arrays.copyOfRange(original,0,BLOCK_SIZE),byteBufferToArray(result));
  }
  finally {
    if (fsIn != null)     fsIn.close();
    if (fs != null)     fs.close();
    if (cluster != null)     cluster.shutdown();
  }
}
