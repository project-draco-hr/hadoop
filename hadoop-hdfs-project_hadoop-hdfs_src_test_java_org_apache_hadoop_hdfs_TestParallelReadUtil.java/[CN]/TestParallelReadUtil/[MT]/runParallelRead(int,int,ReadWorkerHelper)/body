{
  ReadWorker workers[]=new ReadWorker[nFiles * nWorkerEach];
  TestFileInfo testInfoArr[]=new TestFileInfo[nFiles];
  int nWorkers=0;
  for (int i=0; i < nFiles; ++i) {
    TestFileInfo testInfo=new TestFileInfo();
    testInfoArr[i]=testInfo;
    testInfo.filepath=new Path("/TestParallelRead.dat." + i);
    testInfo.authenticData=util.writeFile(testInfo.filepath,FILE_SIZE_K);
    testInfo.dis=dfsClient.open(testInfo.filepath.toString(),dfsClient.getConf().ioBufferSize,verifyChecksums);
    for (int j=0; j < nWorkerEach; ++j) {
      workers[nWorkers++]=new ReadWorker(testInfo,nWorkers,helper);
    }
  }
  long starttime=Time.now();
  for (  ReadWorker worker : workers) {
    worker.start();
  }
  for (  ReadWorker worker : workers) {
    try {
      worker.join();
    }
 catch (    InterruptedException ignored) {
    }
  }
  long endtime=Time.now();
  for (  TestFileInfo testInfo : testInfoArr) {
    testInfo.dis.close();
  }
  boolean res=true;
  long totalRead=0;
  for (  ReadWorker worker : workers) {
    long nread=worker.getBytesRead();
    LOG.info("--- Report: " + worker.getName() + " read "+ nread+ " B; "+ "average "+ nread / ReadWorker.N_ITERATIONS + " B per read");
    totalRead+=nread;
    if (worker.hasError()) {
      res=false;
    }
  }
  double timeTakenSec=(endtime - starttime) / 1000.0;
  long totalReadKB=totalRead / 1024;
  LOG.info("=== Report: " + nWorkers + " threads read "+ totalReadKB+ " KB (across "+ nFiles+ " file(s)) in "+ timeTakenSec+ "s; average "+ totalReadKB / timeTakenSec + " KB/s");
  return res;
}
