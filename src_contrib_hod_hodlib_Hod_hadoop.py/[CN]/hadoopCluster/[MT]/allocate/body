def allocate(self, clusterDir, min, max=None):
    status = 0
    failureCount = 0
    self.__svcrgyClient = self.__get_svcrgy_client()
    self.__log.debug(('allocate %s %s %s' % (clusterDir, min, max)))
    if (min < 3):
        self.__log.critical('Minimum nodes must be greater than 2.')
        status = 2
    else:
        nodeSet = self.__nodePool.newNodeSet(min)
        walltime = None
        if self.__cfg['hod'].has_key('walltime'):
            walltime = self.__cfg['hod']['walltime']
        (self.jobId, exitCode) = self.__nodePool.submitNodeSet(nodeSet, walltime)
        while ((self.jobId is False) and (exitCode != 188)):
            if hodInterrupt.isSet():
                raise HodInterruptException()
            failureCount += 1
            if (failureCount >= self.__cfg['hod']['job-status-query-failure-retries']):
                self.__log.debug('failed submitting job more than the retries. exiting')
                break
            else:
                time.sleep(self.__cfg['hod']['job-command-failure-interval'])
                if hodInterrupt.isSet():
                    raise HodInterruptException()
                (self.jobId, exitCode) = self.__nodePool.submitNodeSet(nodeSet, walltime)
        if self.jobId:
            jobStatus = None
            try:
                jobStatus = self.__check_job_status()
            except HodInterruptException as h:
                self.__log.info(HOD_INTERRUPTED_MESG)
                self.delete_job(self.jobId)
                self.__log.info(('Cluster %s removed from queue.' % self.jobId))
                raise h
            else:
                if (jobStatus == (-1)):
                    self.delete_job(self.jobId)
                    status = 4
                    return status
            if jobStatus:
                self.__log.info(('Cluster Id %s' % self.jobId))
                try:
                    self.ringmasterXRS = self.__get_ringmaster_client()
                    self.__log.debug(('Ringmaster at : %s' % self.ringmasterXRS))
                    ringClient = None
                    if self.ringmasterXRS:
                        ringClient = hodXRClient(self.ringmasterXRS)
                        (hdfsStatus, hdfsAddr, self.hdfsInfo) = self.__init_hadoop_service('hdfs', ringClient)
                        if hdfsStatus:
                            self.__log.info(('HDFS UI at http://%s' % self.hdfsInfo))
                            (mapredStatus, mapredAddr, self.mapredInfo) = self.__init_hadoop_service('mapred', ringClient)
                            if mapredStatus:
                                self.__log.info(('Mapred UI at http://%s' % self.mapredInfo))
                                if (self.__cfg['hod'].has_key('update-worker-info') and self.__cfg['hod']['update-worker-info']):
                                    workerInfoMap = {}
                                    workerInfoMap['HDFS UI'] = ('http://%s' % self.hdfsInfo)
                                    workerInfoMap['Mapred UI'] = ('http://%s' % self.mapredInfo)
                                    workerInfoMap['RM RPC Port'] = ('%s' % self.ringmasterXRS.split(':')[2].strip('/'))
                                    if (mapredAddr.find(':') != (-1)):
                                        workerInfoMap['Mapred RPC Port'] = mapredAddr.split(':')[1]
                                    ret = self.__nodePool.updateWorkerInfo(workerInfoMap, self.jobId)
                                    if (ret != 0):
                                        self.__log.warn(('Could not update HDFS and Mapred information.User Portal may not show relevant information.Error code=%s' % ret))
                                self.__cfg.replace_escape_seqs()
                                clientParams = None
                                serverParams = {}
                                finalServerParams = {}
                                if self.__cfg['hod'].has_key('client-params'):
                                    clientParams = self.__cfg['hod']['client-params']
                                if self.__cfg['gridservice-mapred'].has_key('server-params'):
                                    serverParams.update(self.__cfg['gridservice-mapred']['server-params'])
                                if self.__cfg['gridservice-hdfs'].has_key('server-params'):
                                    serverParams.update(self.__cfg['gridservice-hdfs']['server-params'])
                                if self.__cfg['gridservice-mapred'].has_key('final-server-params'):
                                    finalServerParams.update(self.__cfg['gridservice-mapred']['final-server-params'])
                                if self.__cfg['gridservice-hdfs'].has_key('final-server-params'):
                                    finalServerParams.update(self.__cfg['gridservice-hdfs']['final-server-params'])
                                clusterFactor = self.__cfg['hod']['cluster-factor']
                                tempDir = self.__cfg['hod']['temp-dir']
                                if (not os.path.exists(tempDir)):
                                    os.makedirs(tempDir)
                                tempDir = os.path.join(tempDir, ((self.__cfg['hod']['userid'] + '.') + self.jobId))
                                mrSysDir = getMapredSystemDirectory(self.__cfg['hodring']['mapred-system-dir-root'], self.__cfg['hod']['userid'], self.jobId)
                                self.__hadoopCfg.gen_site_conf(clusterDir, tempDir, min, hdfsAddr, mrSysDir, mapredAddr, clientParams, serverParams, finalServerParams, clusterFactor)
                                self.__log.info(('hadoop-site.xml at %s' % clusterDir))
                            else:
                                status = 8
                        else:
                            status = 7
                    else:
                        status = 6
                    if (status != 0):
                        self.__log.debug(('Cleaning up cluster id %s, as cluster could not be allocated.' % self.jobId))
                        if (ringClient is None):
                            self.delete_job(self.jobId)
                        else:
                            self.__log.debug('Calling rm.stop()')
                            ringClient.stopRM()
                            self.__log.debug('Returning from rm.stop()')
                except HodInterruptException as h:
                    self.__log.info(HOD_INTERRUPTED_MESG)
                    if self.ringmasterXRS:
                        if (ringClient is None):
                            ringClient = hodXRClient(self.ringmasterXRS)
                        self.__log.debug('Calling rm.stop()')
                        ringClient.stopRM()
                        self.__log.debug('Returning from rm.stop()')
                        self.__log.info('Cluster Shutdown by informing ringmaster.')
                    else:
                        self.delete_job(self.jobId)
                        self.__log.info(('Cluster %s removed from queue directly.' % self.jobId))
                    raise h
            else:
                self.__log.critical('No cluster found, ringmaster failed to run.')
                status = 5
        elif (self.jobId == False):
            if (exitCode == 188):
                self.__log.critical('Request execeeded maximum resource allocation.')
            else:
                self.__log.critical(('Job submission failed with exit code %s' % exitCode))
            status = 4
        else:
            self.__log.critical('Scheduler failure, allocation failed.\n\n')
            status = 4
    if ((status == 5) or (status == 6)):
        ringMasterErrors = self.__svcrgyClient.getRMError()
        if ringMasterErrors:
            self.__log.critical(('Cluster could not be allocated because of the following errors on the ringmaster host %s.\n%s' % (ringMasterErrors[0], ringMasterErrors[1])))
            self.__log.debug(('Stack trace on ringmaster: %s' % ringMasterErrors[2]))
    return status
