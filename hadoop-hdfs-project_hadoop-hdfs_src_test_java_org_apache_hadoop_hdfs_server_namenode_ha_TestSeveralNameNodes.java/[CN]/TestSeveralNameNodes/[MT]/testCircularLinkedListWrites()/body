{
  HAStressTestHarness harness=new HAStressTestHarness();
  harness.setNumberOfNameNodes(NUM_NAMENODES);
  harness.addFailoverThread(TIME_BETWEEN_FAILOVERS);
  harness.conf.setInt(HdfsClientConfigKeys.Failover.SLEEPTIME_MAX_KEY,1000);
  harness.conf.setInt(HdfsClientConfigKeys.Failover.MAX_ATTEMPTS_KEY,128);
  final MiniDFSCluster cluster=harness.startCluster();
  try {
    cluster.waitActive();
    cluster.transitionToActive(0);
    FileSystem fs=harness.getFailoverFs();
    TestContext context=harness.testCtx;
    List<CircularWriter> writers=new ArrayList<CircularWriter>();
    for (int i=0; i < NUM_THREADS; i++) {
      Path p=new Path("/test-" + i);
      fs.mkdirs(p);
      CircularWriter writer=new CircularWriter(context,LIST_LENGTH,fs,p);
      writers.add(writer);
      context.addThread(writer);
    }
    harness.startThreads();
    long start=System.currentTimeMillis();
    while ((System.currentTimeMillis() - start) < RUNTIME && writers.size() > 0) {
      for (int i=0; i < writers.size(); i++) {
        CircularWriter writer=writers.get(i);
        if (writer.done.await(100,TimeUnit.MILLISECONDS)) {
          writers.remove(i--);
        }
      }
    }
    assertEquals("Some writers didn't complete in expected runtime! Current writer state:" + writers,0,writers.size());
    harness.stopThreads();
  }
  finally {
    System.err.println("===========================\n\n\n\n");
    harness.shutdown();
  }
}
