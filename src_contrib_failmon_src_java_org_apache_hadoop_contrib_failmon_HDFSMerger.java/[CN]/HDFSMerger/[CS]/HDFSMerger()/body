{
  String hadoopConfPath;
  if (Environment.getProperty("hadoop.conf.path") == null)   hadoopConfPath="../../../conf";
 else   hadoopConfPath=Environment.getProperty("hadoop.conf.path");
  Configuration hadoopConf=new Configuration();
  hadoopConf.addResource(new Path(hadoopConfPath + "/hadoop-default.xml"));
  hadoopConf.addResource(new Path(hadoopConfPath + "/hadoop-site.xml"));
  if (Environment.getProperty("local.tmp.filename") == null)   Environment.setProperty("local.tmp.filename","failmon.dat");
  hdfsDir=Environment.getProperty("hdfs.upload.dir");
  if (hdfsDir == null)   hdfsDir="/failmon";
  hdfs=FileSystem.get(hadoopConf);
  Path hdfsDirPath=new Path(hadoopConf.get("fs.default.name") + hdfsDir);
  try {
    if (!hdfs.getFileStatus(hdfsDirPath).isDir()) {
      Environment.logInfo("HDFSMerger: Not an HDFS directory: " + hdfsDirPath.toString());
      System.exit(0);
    }
  }
 catch (  FileNotFoundException e) {
    Environment.logInfo("HDFSMerger: Directory not found: " + hdfsDirPath.toString());
  }
  inputFiles=hdfs.listStatus(hdfsDirPath);
  outputFilePath=new Path(hdfsDirPath.toString() + "/" + "merge-"+ Calendar.getInstance().getTimeInMillis()+ ".dat");
  outputFile=hdfs.create(outputFilePath);
  for (  FileStatus fstatus : inputFiles) {
    appendFile(fstatus.getPath());
    hdfs.delete(fstatus.getPath(),true);
  }
  outputFile.close();
  Environment.logInfo("HDFS file merging complete!");
}
