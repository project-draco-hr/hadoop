{
  LOG.info("Status update from " + taskAttemptID.toString());
  org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId yarnAttemptID=TypeConverter.toYarn(taskAttemptID);
  taskHeartbeatHandler.progressing(yarnAttemptID);
  TaskAttemptStatus taskAttemptStatus=new TaskAttemptStatus();
  taskAttemptStatus.id=yarnAttemptID;
  taskAttemptStatus.progress=taskStatus.getProgress();
  LOG.info("Progress of TaskAttempt " + taskAttemptID + " is : "+ taskStatus.getProgress());
  taskAttemptStatus.stateString=taskStatus.getStateString();
  taskAttemptStatus.outputSize=taskStatus.getOutputSize();
  taskAttemptStatus.phase=TypeConverter.toYarn(taskStatus.getPhase());
  taskAttemptStatus.counters=new org.apache.hadoop.mapreduce.Counters(taskStatus.getCounters());
  if (taskStatus.getIsMap() && taskStatus.getMapFinishTime() != 0) {
    taskAttemptStatus.mapFinishTime=taskStatus.getMapFinishTime();
  }
  if (!taskStatus.getIsMap() && taskStatus.getShuffleFinishTime() != 0) {
    taskAttemptStatus.shuffleFinishTime=taskStatus.getShuffleFinishTime();
  }
  if (!taskStatus.getIsMap() && taskStatus.getSortFinishTime() != 0) {
    taskAttemptStatus.sortFinishTime=taskStatus.getSortFinishTime();
  }
  if (taskStatus.getFetchFailedMaps() != null && taskStatus.getFetchFailedMaps().size() > 0) {
    taskAttemptStatus.fetchFailedMaps=new ArrayList<org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId>();
    for (    TaskAttemptID failedMapId : taskStatus.getFetchFailedMaps()) {
      taskAttemptStatus.fetchFailedMaps.add(TypeConverter.toYarn(failedMapId));
    }
  }
  context.getEventHandler().handle(new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id,taskAttemptStatus));
  return true;
}
