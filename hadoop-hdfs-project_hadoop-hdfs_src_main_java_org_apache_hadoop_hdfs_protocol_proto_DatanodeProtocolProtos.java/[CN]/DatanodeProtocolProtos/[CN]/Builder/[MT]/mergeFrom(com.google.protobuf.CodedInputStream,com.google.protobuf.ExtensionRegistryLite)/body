{
  com.google.protobuf.UnknownFieldSet.Builder unknownFields=com.google.protobuf.UnknownFieldSet.newBuilder(this.getUnknownFields());
  while (true) {
    int tag=input.readTag();
switch (tag) {
case 0:
      this.setUnknownFields(unknownFields.build());
    onChanged();
  return this;
default :
{
  if (!parseUnknownField(input,unknownFields,extensionRegistry,tag)) {
    this.setUnknownFields(unknownFields.build());
    onChanged();
    return this;
  }
  break;
}
case 8:
{
int rawValue=input.readEnum();
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.DatanodeCommandProto.Type value=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.DatanodeCommandProto.Type.valueOf(rawValue);
if (value == null) {
  unknownFields.mergeVarintField(1,rawValue);
}
 else {
  bitField0_|=0x00000001;
  cmdType_=value;
}
break;
}
case 18:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BalancerBandwidthCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BalancerBandwidthCommandProto.newBuilder();
if (hasBalancerCmd()) {
subBuilder.mergeFrom(getBalancerCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setBalancerCmd(subBuilder.buildPartial());
break;
}
case 26:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockCommandProto.newBuilder();
if (hasBlkCmd()) {
subBuilder.mergeFrom(getBlkCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setBlkCmd(subBuilder.buildPartial());
break;
}
case 34:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.BlockRecoveryCommndProto.newBuilder();
if (hasRecoveryCmd()) {
subBuilder.mergeFrom(getRecoveryCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setRecoveryCmd(subBuilder.buildPartial());
break;
}
case 42:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.FinalizeCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.FinalizeCommandProto.newBuilder();
if (hasFinalizeCmd()) {
subBuilder.mergeFrom(getFinalizeCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setFinalizeCmd(subBuilder.buildPartial());
break;
}
case 50:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.KeyUpdateCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.KeyUpdateCommandProto.newBuilder();
if (hasKeyUpdateCmd()) {
subBuilder.mergeFrom(getKeyUpdateCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setKeyUpdateCmd(subBuilder.buildPartial());
break;
}
case 58:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.RegisterCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.RegisterCommandProto.newBuilder();
if (hasRegisterCmd()) {
subBuilder.mergeFrom(getRegisterCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setRegisterCmd(subBuilder.buildPartial());
break;
}
case 66:
{
org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.Builder subBuilder=org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos.UpgradeCommandProto.newBuilder();
if (hasUpgradeCmd()) {
subBuilder.mergeFrom(getUpgradeCmd());
}
input.readMessage(subBuilder,extensionRegistry);
setUpgradeCmd(subBuilder.buildPartial());
break;
}
}
}
}
