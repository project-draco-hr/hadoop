{
  Configuration conf=new HdfsConfiguration();
  TestBalancer.initConf(conf);
  long newNodeCapacity=TestBalancer.CAPACITY;
  String newNodeRack=TestBalancer.RACK2;
  String[] racks=new String[]{TestBalancer.RACK0,TestBalancer.RACK1};
  long[] capacities=new long[]{TestBalancer.CAPACITY,TestBalancer.CAPACITY};
  assertEquals(capacities.length,racks.length);
  int numOfDatanodes=capacities.length;
  NNConf nn1Conf=new MiniDFSNNTopology.NNConf("nn1");
  nn1Conf.setIpcPort(NameNode.DEFAULT_PORT);
  Configuration copiedConf=new Configuration(conf);
  cluster=new MiniDFSCluster.Builder(copiedConf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(capacities.length).racks(racks).simulatedCapacities(capacities).build();
  HATestUtil.setFailoverConfigurations(cluster,conf);
  try {
    cluster.waitActive();
    cluster.transitionToActive(1);
    Thread.sleep(500);
    client=NameNodeProxies.createProxy(conf,FileSystem.getDefaultUri(conf),ClientProtocol.class).getProxy();
    long totalCapacity=TestBalancer.sum(capacities);
    long totalUsedSpace=totalCapacity * 3 / 10;
    TestBalancer.createFile(cluster,TestBalancer.filePath,totalUsedSpace / numOfDatanodes,(short)numOfDatanodes,1);
    cluster.startDataNodes(conf,1,true,null,new String[]{newNodeRack},new long[]{newNodeCapacity});
    totalCapacity+=newNodeCapacity;
    TestBalancer.waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
    Collection<URI> namenodes=DFSUtil.getNsServiceRpcUris(conf);
    assertEquals(1,namenodes.size());
    assertTrue(namenodes.contains(HATestUtil.getLogicalUri(cluster)));
    final int r=Balancer.run(namenodes,Balancer.Parameters.DEFAULT,conf);
    assertEquals(Balancer.ReturnStatus.SUCCESS.code,r);
    TestBalancer.waitForBalancer(totalUsedSpace,totalCapacity,client,cluster,Balancer.Parameters.DEFAULT);
  }
  finally {
    cluster.shutdown();
  }
}
