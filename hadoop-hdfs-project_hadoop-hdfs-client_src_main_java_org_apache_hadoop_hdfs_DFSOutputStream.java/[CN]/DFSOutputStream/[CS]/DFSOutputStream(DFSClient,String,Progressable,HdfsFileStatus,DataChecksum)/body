{
  super(getChecksum4Compute(checksum,stat));
  this.dfsClient=dfsClient;
  this.src=src;
  this.fileId=stat.getFileId();
  this.blockSize=stat.getBlockSize();
  this.blockReplication=stat.getReplication();
  this.fileEncryptionInfo=stat.getFileEncryptionInfo();
  this.cachingStrategy=new AtomicReference<CachingStrategy>(dfsClient.getDefaultWriteCachingStrategy());
  if (progress != null) {
    DFSClient.LOG.debug("Set non-null progress callback on DFSOutputStream " + "{}",src);
  }
  this.bytesPerChecksum=checksum.getBytesPerChecksum();
  if (bytesPerChecksum <= 0) {
    throw new HadoopIllegalArgumentException("Invalid value: bytesPerChecksum = " + bytesPerChecksum + " <= 0");
  }
  if (blockSize % bytesPerChecksum != 0) {
    throw new HadoopIllegalArgumentException("Invalid values: " + HdfsClientConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY + " (="+ bytesPerChecksum+ ") must divide block size (="+ blockSize+ ").");
  }
  this.byteArrayManager=dfsClient.getClientContext().getByteArrayManager();
}
