{
  JobConf jobConf=new JobConf();
  ShuffleConsumerPlugin<K,V> shuffleConsumerPlugin=new TestShuffleConsumerPlugin<K,V>();
  ReduceTask mockReduceTask=mock(ReduceTask.class);
  TaskUmbilicalProtocol mockUmbilical=mock(TaskUmbilicalProtocol.class);
  Reporter mockReporter=mock(Reporter.class);
  FileSystem mockFileSystem=mock(FileSystem.class);
  Class<? extends org.apache.hadoop.mapred.Reducer> combinerClass=jobConf.getCombinerClass();
  @SuppressWarnings("unchecked") CombineOutputCollector<K,V> mockCombineOutputCollector=(CombineOutputCollector<K,V>)mock(CombineOutputCollector.class);
  org.apache.hadoop.mapreduce.TaskAttemptID mockTaskAttemptID=mock(org.apache.hadoop.mapreduce.TaskAttemptID.class);
  LocalDirAllocator mockLocalDirAllocator=mock(LocalDirAllocator.class);
  CompressionCodec mockCompressionCodec=mock(CompressionCodec.class);
  Counter mockCounter=mock(Counter.class);
  TaskStatus mockTaskStatus=mock(TaskStatus.class);
  Progress mockProgress=mock(Progress.class);
  MapOutputFile mockMapOutputFile=mock(MapOutputFile.class);
  Task mockTask=mock(Task.class);
  try {
    String[] dirs=jobConf.getLocalDirs();
    ShuffleConsumerPlugin.Context<K,V> context=new ShuffleConsumerPlugin.Context<K,V>(mockTaskAttemptID,jobConf,mockFileSystem,mockUmbilical,mockLocalDirAllocator,mockReporter,mockCompressionCodec,combinerClass,mockCombineOutputCollector,mockCounter,mockCounter,mockCounter,mockCounter,mockCounter,mockCounter,mockTaskStatus,mockProgress,mockProgress,mockTask,mockMapOutputFile,null);
    shuffleConsumerPlugin.init(context);
    shuffleConsumerPlugin.run();
    shuffleConsumerPlugin.close();
  }
 catch (  Exception e) {
    assertTrue("Threw exception:" + e,false);
  }
  mockReduceTask.getTaskID();
  mockReduceTask.getJobID();
  mockReduceTask.getNumMaps();
  mockReduceTask.getPartition();
  mockReporter.progress();
}
