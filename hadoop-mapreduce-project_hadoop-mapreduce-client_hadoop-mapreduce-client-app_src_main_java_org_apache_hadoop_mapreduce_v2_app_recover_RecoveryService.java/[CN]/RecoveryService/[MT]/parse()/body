{
  FSDataInputStream in=getPreviousJobHistoryFileStream(getConfig(),applicationAttemptId);
  JobHistoryParser parser=new JobHistoryParser(in);
  jobInfo=parser.parse();
  Exception parseException=parser.getParseException();
  if (parseException != null) {
    LOG.info("Got an error parsing job-history file" + ", ignoring incomplete events.",parseException);
  }
  Map<org.apache.hadoop.mapreduce.TaskID,TaskInfo> taskInfos=jobInfo.getAllTasks();
  for (  TaskInfo taskInfo : taskInfos.values()) {
    if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {
      Iterator<Entry<TaskAttemptID,TaskAttemptInfo>> taskAttemptIterator=taskInfo.getAllTaskAttempts().entrySet().iterator();
      while (taskAttemptIterator.hasNext()) {
        Map.Entry<TaskAttemptID,TaskAttemptInfo> currentEntry=taskAttemptIterator.next();
        if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {
          taskAttemptIterator.remove();
        }
      }
      completedTasks.put(TypeConverter.toYarn(taskInfo.getTaskId()),taskInfo);
      LOG.info("Read from history task " + TypeConverter.toYarn(taskInfo.getTaskId()));
    }
  }
  LOG.info("Read completed tasks from history " + completedTasks.size());
}
