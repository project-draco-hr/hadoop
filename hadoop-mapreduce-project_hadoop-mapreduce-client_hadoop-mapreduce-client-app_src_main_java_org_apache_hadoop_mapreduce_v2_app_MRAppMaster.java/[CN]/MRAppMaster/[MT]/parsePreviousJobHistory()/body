{
  FSDataInputStream in=getPreviousJobHistoryStream(getConfig(),appAttemptID);
  JobHistoryParser parser=new JobHistoryParser(in);
  JobInfo jobInfo=parser.parse();
  Exception parseException=parser.getParseException();
  if (parseException != null) {
    LOG.info("Got an error parsing job-history file" + ", ignoring incomplete events.",parseException);
  }
  Map<org.apache.hadoop.mapreduce.TaskID,TaskInfo> taskInfos=jobInfo.getAllTasks();
  for (  TaskInfo taskInfo : taskInfos.values()) {
    if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {
      Iterator<Entry<TaskAttemptID,TaskAttemptInfo>> taskAttemptIterator=taskInfo.getAllTaskAttempts().entrySet().iterator();
      while (taskAttemptIterator.hasNext()) {
        Map.Entry<TaskAttemptID,TaskAttemptInfo> currentEntry=taskAttemptIterator.next();
        if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {
          taskAttemptIterator.remove();
        }
      }
      completedTasksFromPreviousRun.put(TypeConverter.toYarn(taskInfo.getTaskId()),taskInfo);
      LOG.info("Read from history task " + TypeConverter.toYarn(taskInfo.getTaskId()));
    }
  }
  LOG.info("Read completed tasks from history " + completedTasksFromPreviousRun.size());
  recoveredJobStartTime=jobInfo.getLaunchTime();
  List<JobHistoryParser.AMInfo> jhAmInfoList=jobInfo.getAMInfos();
  if (jhAmInfoList != null) {
    for (    JobHistoryParser.AMInfo jhAmInfo : jhAmInfoList) {
      AMInfo amInfo=MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(),jhAmInfo.getStartTime(),jhAmInfo.getContainerId(),jhAmInfo.getNodeManagerHost(),jhAmInfo.getNodeManagerPort(),jhAmInfo.getNodeManagerHttpPort());
      amInfos.add(amInfo);
    }
  }
}
