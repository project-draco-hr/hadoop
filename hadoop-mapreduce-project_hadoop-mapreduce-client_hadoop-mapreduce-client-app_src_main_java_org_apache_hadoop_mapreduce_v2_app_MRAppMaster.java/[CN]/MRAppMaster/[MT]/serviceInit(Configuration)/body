{
  conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY,true);
  initJobCredentialsAndUGI(conf);
  isLastAMRetry=appAttemptID.getAttemptId() >= maxAppAttempts;
  LOG.info("The specific max attempts: " + maxAppAttempts + " for application: "+ appAttemptID.getApplicationId().getId()+ ". Attempt num: "+ appAttemptID.getAttemptId()+ " is last retry: "+ isLastAMRetry);
  context=new RunningAppContext(conf);
  appName=conf.get(MRJobConfig.JOB_NAME,"<missing app name>");
  conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID,appAttemptID.getAttemptId());
  newApiCommitter=false;
  jobId=MRBuilderUtils.newJobId(appAttemptID.getApplicationId(),appAttemptID.getApplicationId().getId());
  int numReduceTasks=conf.getInt(MRJobConfig.NUM_REDUCES,0);
  if ((numReduceTasks > 0 && conf.getBoolean("mapred.reducer.new-api",false)) || (numReduceTasks == 0 && conf.getBoolean("mapred.mapper.new-api",false))) {
    newApiCommitter=true;
    LOG.info("Using mapred newApiCommitter.");
  }
  boolean copyHistory=false;
  try {
    String user=UserGroupInformation.getCurrentUser().getShortUserName();
    Path stagingDir=MRApps.getStagingAreaDir(conf,user);
    FileSystem fs=getFileSystem(conf);
    boolean stagingExists=fs.exists(stagingDir);
    Path startCommitFile=MRApps.getStartJobCommitFile(conf,user,jobId);
    boolean commitStarted=fs.exists(startCommitFile);
    Path endCommitSuccessFile=MRApps.getEndJobCommitSuccessFile(conf,user,jobId);
    boolean commitSuccess=fs.exists(endCommitSuccessFile);
    Path endCommitFailureFile=MRApps.getEndJobCommitFailureFile(conf,user,jobId);
    boolean commitFailure=fs.exists(endCommitFailureFile);
    if (!stagingExists) {
      isLastAMRetry=true;
      LOG.info("Attempt num: " + appAttemptID.getAttemptId() + " is last retry: "+ isLastAMRetry+ " because the staging dir doesn't exist.");
      errorHappenedShutDown=true;
      forcedState=JobStateInternal.ERROR;
      shutDownMessage="Staging dir does not exist " + stagingDir;
      LOG.fatal(shutDownMessage);
    }
 else     if (commitStarted) {
      errorHappenedShutDown=true;
      isLastAMRetry=true;
      LOG.info("Attempt num: " + appAttemptID.getAttemptId() + " is last retry: "+ isLastAMRetry+ " because a commit was started.");
      copyHistory=true;
      if (commitSuccess) {
        shutDownMessage="We crashed after successfully committing. Recovering.";
        forcedState=JobStateInternal.SUCCEEDED;
      }
 else       if (commitFailure) {
        shutDownMessage="We crashed after a commit failure.";
        forcedState=JobStateInternal.FAILED;
      }
 else {
        shutDownMessage="We crashed durring a commit";
        forcedState=JobStateInternal.ERROR;
      }
    }
  }
 catch (  IOException e) {
    throw new YarnRuntimeException("Error while initializing",e);
  }
  if (errorHappenedShutDown) {
    dispatcher=createDispatcher();
    addIfService(dispatcher);
    NoopEventHandler eater=new NoopEventHandler();
    dispatcher.register(JobEventType.class,eater);
    EventHandler<JobHistoryEvent> historyService=null;
    if (copyHistory) {
      historyService=createJobHistoryHandler(context);
      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,historyService);
    }
 else {
      dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,eater);
    }
    containerAllocator=createContainerAllocator(null,context);
    addIfService(containerAllocator);
    dispatcher.register(ContainerAllocator.EventType.class,containerAllocator);
    if (copyHistory) {
      addService(createStagingDirCleaningService());
      addIfService(historyService);
      JobHistoryCopyService cpHist=new JobHistoryCopyService(appAttemptID,dispatcher.getEventHandler());
      addIfService(cpHist);
    }
  }
 else {
    committer=createOutputCommitter(conf);
    dispatcher=createDispatcher();
    addIfService(dispatcher);
    clientService=createClientService(context);
    addIfService(clientService);
    containerAllocator=createContainerAllocator(clientService,context);
    committerEventHandler=createCommitterEventHandler(context,committer);
    addIfService(committerEventHandler);
    taskAttemptListener=createTaskAttemptListener(context);
    addIfService(taskAttemptListener);
    EventHandler<JobHistoryEvent> historyService=createJobHistoryHandler(context);
    dispatcher.register(org.apache.hadoop.mapreduce.jobhistory.EventType.class,historyService);
    this.jobEventDispatcher=new JobEventDispatcher();
    dispatcher.register(JobEventType.class,jobEventDispatcher);
    dispatcher.register(TaskEventType.class,new TaskEventDispatcher());
    dispatcher.register(TaskAttemptEventType.class,new TaskAttemptEventDispatcher());
    dispatcher.register(CommitterEventType.class,committerEventHandler);
    if (conf.getBoolean(MRJobConfig.MAP_SPECULATIVE,false) || conf.getBoolean(MRJobConfig.REDUCE_SPECULATIVE,false)) {
      speculator=createSpeculator(conf,context);
      addIfService(speculator);
    }
    speculatorEventDispatcher=new SpeculatorEventDispatcher(conf);
    dispatcher.register(Speculator.EventType.class,speculatorEventDispatcher);
    addIfService(containerAllocator);
    dispatcher.register(ContainerAllocator.EventType.class,containerAllocator);
    containerLauncher=createContainerLauncher(context);
    addIfService(containerLauncher);
    dispatcher.register(ContainerLauncher.EventType.class,containerLauncher);
    addService(createStagingDirCleaningService());
    addIfService(historyService);
  }
  super.serviceInit(conf);
}
