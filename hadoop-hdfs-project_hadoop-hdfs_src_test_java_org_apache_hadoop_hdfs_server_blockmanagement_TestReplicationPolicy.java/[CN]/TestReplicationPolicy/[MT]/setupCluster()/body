{
  Configuration conf=new HdfsConfiguration();
  final String[] racks={"/d1/r1","/d1/r1","/d1/r2","/d1/r2","/d2/r3","/d2/r3"};
  storages=DFSTestUtil.createDatanodeStorageInfos(racks);
  dataNodes=DFSTestUtil.toDatanodeDescriptor(storages);
  FileSystem.setDefaultUri(conf,"hdfs://localhost:0");
  conf.set(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY,"0.0.0.0:0");
  File baseDir=PathUtils.getTestDir(TestReplicationPolicy.class);
  conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,new File(baseDir,"name").getPath());
  conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_AVOID_STALE_DATANODE_FOR_READ_KEY,true);
  conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_AVOID_STALE_DATANODE_FOR_WRITE_KEY,true);
  DFSTestUtil.formatNameNode(conf);
  namenode=new NameNode(conf);
  final BlockManager bm=namenode.getNamesystem().getBlockManager();
  replicator=bm.getBlockPlacementPolicy();
  cluster=bm.getDatanodeManager().getNetworkTopology();
  for (int i=0; i < NUM_OF_DATANODES; i++) {
    cluster.add(dataNodes[i]);
    bm.getDatanodeManager().getHeartbeatManager().addDatanode(dataNodes[i]);
  }
  for (int i=0; i < NUM_OF_DATANODES; i++) {
    updateHeartbeatWithUsage(dataNodes[i],2 * HdfsConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,2 * HdfsConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,0,0);
  }
}
