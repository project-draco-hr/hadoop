{
  Path file1=new Path("/checkpoint.dat");
  Path file2=new Path("/checkpoint2.dat");
  Path file3=new Path("/backup.dat");
  Configuration conf=new HdfsConfiguration();
  HAUtil.setAllowStandbyReads(conf,true);
  short replication=(short)conf.getInt("dfs.replication",3);
  int numDatanodes=Math.max(3,replication);
  conf.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,"localhost:0");
  conf.set(DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY,"0");
  conf.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,-1);
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,1);
  MiniDFSCluster cluster=null;
  FileSystem fileSys=null;
  BackupNode backup=null;
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    fileSys=cluster.getFileSystem();
    assertTrue(!fileSys.exists(file1));
    assertTrue(!fileSys.exists(file2));
    assertTrue(fileSys.mkdirs(file1));
    long txid=cluster.getNameNodeRpc().getTransactionID();
    backup=startBackupNode(conf,op,1);
    waitCheckpointDone(cluster,txid);
  }
 catch (  IOException e) {
    LOG.error("Error in TestBackupNode:",e);
    assertTrue(e.getLocalizedMessage(),false);
  }
 finally {
    if (backup != null)     backup.stop();
    if (fileSys != null)     fileSys.close();
    if (cluster != null)     cluster.shutdown();
  }
  File nnCurDir=new File(BASE_DIR,"name1/current/");
  File bnCurDir=new File(getBackupNodeDir(op,1),"/current/");
  FSImageTestUtil.assertParallelFilesAreIdentical(ImmutableList.of(bnCurDir,nnCurDir),ImmutableSet.<String>of("VERSION"));
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
    fileSys=cluster.getFileSystem();
    assertTrue(fileSys.exists(file1));
    fileSys.delete(file1,true);
    fileSys.mkdirs(file2);
    long txid=cluster.getNameNodeRpc().getTransactionID();
    backup=startBackupNode(conf,op,1);
    waitCheckpointDone(cluster,txid);
    for (int i=0; i < 10; i++) {
      fileSys.mkdirs(new Path("file_" + i));
    }
    txid=cluster.getNameNodeRpc().getTransactionID();
    backup.doCheckpoint();
    waitCheckpointDone(cluster,txid);
    txid=cluster.getNameNodeRpc().getTransactionID();
    backup.doCheckpoint();
    waitCheckpointDone(cluster,txid);
    InetSocketAddress add=backup.getNameNodeAddress();
    FileSystem bnFS=FileSystem.get(new Path("hdfs://" + NetUtils.getHostPortString(add)).toUri(),conf);
    boolean canWrite=true;
    try {
      DFSTestUtil.createFile(bnFS,file3,fileSize,fileSize,blockSize,replication,seed);
    }
 catch (    IOException eio) {
      LOG.info("Write to " + backup.getRole() + " failed as expected: ",eio);
      canWrite=false;
    }
    assertFalse("Write to BackupNode must be prohibited.",canWrite);
    boolean canRead=true;
    try {
      bnFS.exists(file2);
    }
 catch (    IOException eio) {
      LOG.info("Read from " + backup.getRole() + " failed: ",eio);
      canRead=false;
    }
    assertEquals("Reads to BackupNode are allowed, but not CheckpointNode.",canRead,backup.isRole(NamenodeRole.BACKUP));
    DFSTestUtil.createFile(fileSys,file3,fileSize,fileSize,blockSize,replication,seed);
    TestCheckpoint.checkFile(fileSys,file3,replication);
    assertTrue("file3 does not exist on BackupNode",op != StartupOption.BACKUP || backup.getNamesystem().getFileInfo(file3.toUri().getPath(),false) != null);
  }
 catch (  IOException e) {
    LOG.error("Error in TestBackupNode:",e);
    throw new AssertionError(e);
  }
 finally {
    if (backup != null)     backup.stop();
    if (fileSys != null)     fileSys.close();
    if (cluster != null)     cluster.shutdown();
  }
  FSImageTestUtil.assertParallelFilesAreIdentical(ImmutableList.of(bnCurDir,nnCurDir),ImmutableSet.<String>of("VERSION"));
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false).build();
    fileSys=cluster.getFileSystem();
    assertTrue(!fileSys.exists(file1));
    assertTrue(fileSys.exists(file2));
  }
 catch (  IOException e) {
    LOG.error("Error in TestBackupNode: ",e);
    assertTrue(e.getLocalizedMessage(),false);
  }
 finally {
    fileSys.close();
    cluster.shutdown();
  }
}
