{
  final int numFiles=4;
  final long fileSize=CACHE_CAPACITY / numFiles;
  final Path[] testFiles=new Path[4];
  final HdfsBlockLocation[][] fileLocs=new HdfsBlockLocation[numFiles][];
  final long[] fileSizes=new long[numFiles];
  for (int i=0; i < numFiles; i++) {
    testFiles[i]=new Path("/testFilesExceedMaxLockedMemory-" + i);
    DFSTestUtil.createFile(fs,testFiles[i],fileSize,(short)1,0xDFAl);
    fileLocs[i]=(HdfsBlockLocation[])fs.getFileBlockLocations(testFiles[i],0,fileSize);
    long[] sizes=getBlockSizes(fileLocs[i]);
    for (int j=0; j < sizes.length; j++) {
      fileSizes[i]+=sizes[j];
    }
  }
  long current=0;
  for (int i=0; i < numFiles - 1; i++) {
    setHeartbeatResponse(cacheBlocks(fileLocs[i]));
    current=verifyExpectedCacheUsage(current + fileSizes[i]);
  }
  final long oldCurrent=current;
  final LogVerificationAppender appender=new LogVerificationAppender();
  final Logger logger=Logger.getRootLogger();
  logger.addAppender(appender);
  setHeartbeatResponse(cacheBlocks(fileLocs[numFiles - 1]));
  int lines=0;
  while (lines == 0) {
    Thread.sleep(100);
    lines=appender.countLinesWithMessage(DFSConfigKeys.DFS_DATANODE_MAX_LOCKED_MEMORY_KEY + " exceeded");
  }
  setHeartbeatResponse(uncacheBlocks(fileLocs[numFiles - 1]));
  while (fsd.getCacheUsed() != oldCurrent) {
    Thread.sleep(100);
  }
  for (int i=0; i < numFiles - 1; i++) {
    setHeartbeatResponse(uncacheBlocks(fileLocs[i]));
    current=verifyExpectedCacheUsage(current - fileSizes[i]);
  }
}
