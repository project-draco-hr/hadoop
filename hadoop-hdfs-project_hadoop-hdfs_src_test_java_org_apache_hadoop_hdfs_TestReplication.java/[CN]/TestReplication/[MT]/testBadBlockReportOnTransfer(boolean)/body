{
  Configuration conf=new HdfsConfiguration();
  FileSystem fs=null;
  DFSClient dfsClient=null;
  LocatedBlocks blocks=null;
  int replicaCount=0;
  short replFactor=1;
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
  cluster.waitActive();
  fs=cluster.getFileSystem();
  dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
  Path file1=new Path("/tmp/testBadBlockReportOnTransfer/file1");
  DFSTestUtil.createFile(fs,file1,1024,replFactor,0);
  DFSTestUtil.waitReplication(fs,file1,replFactor);
  ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,file1);
  int blockFilesCorrupted=corruptBlockByDeletingBlockFile ? cluster.corruptBlockOnDataNodesByDeletingBlockFile(block) : cluster.corruptBlockOnDataNodes(block);
  assertEquals("Corrupted too few blocks",replFactor,blockFilesCorrupted);
  replFactor=2;
  fs.setReplication(file1,replFactor);
  blocks=dfsClient.getNamenode().getBlockLocations(file1.toString(),0,Long.MAX_VALUE);
  while (blocks.get(0).isCorrupt() != true) {
    try {
      LOG.info("Waiting until block is marked as corrupt...");
      Thread.sleep(1000);
    }
 catch (    InterruptedException ie) {
    }
    blocks=dfsClient.getNamenode().getBlockLocations(file1.toString(),0,Long.MAX_VALUE);
  }
  replicaCount=blocks.get(0).getLocations().length;
  assertTrue(replicaCount == 1);
  cluster.shutdown();
}
