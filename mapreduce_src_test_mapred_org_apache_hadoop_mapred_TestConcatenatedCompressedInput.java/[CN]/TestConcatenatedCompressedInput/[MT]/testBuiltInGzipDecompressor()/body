{
  JobConf jobConf=new JobConf(defaultConf);
  jobConf.setBoolean("io.native.lib.available",false);
  CompressionCodec gzip=new GzipCodec();
  ReflectionUtils.setConf(gzip,jobConf);
  localFs.delete(workDir,true);
  assertEquals("[non-native (Java) codec]",org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.class,gzip.getDecompressorType());
  System.out.println(COLOR_BR_YELLOW + "testBuiltInGzipDecompressor() using" + " non-native (Java Inflater) Decompressor ("+ gzip.getDecompressorType()+ ")"+ COLOR_NORMAL);
  String fn1="testConcatThenCompress.txt" + gzip.getDefaultExtension();
  Path fnLocal1=new Path(System.getProperty("test.concat.data","/tmp"),fn1);
  Path fnHDFS1=new Path(workDir,fn1);
  localFs.copyFromLocalFile(fnLocal1,fnHDFS1);
  String fn2="testCompressThenConcat.txt" + gzip.getDefaultExtension();
  Path fnLocal2=new Path(System.getProperty("test.concat.data","/tmp"),fn2);
  Path fnHDFS2=new Path(workDir,fn2);
  localFs.copyFromLocalFile(fnLocal2,fnHDFS2);
  FileInputFormat.setInputPaths(jobConf,workDir);
  final FileInputStream in1=new FileInputStream(fnLocal1.toString());
  final FileInputStream in2=new FileInputStream(fnLocal2.toString());
  assertEquals("concat bytes available",2734,in1.available());
  assertEquals("concat bytes available",3413,in2.available());
  CompressionInputStream cin2=gzip.createInputStream(in2);
  LineReader in=new LineReader(cin2);
  Text out=new Text();
  int numBytes, totalBytes=0, lineNum=0;
  while ((numBytes=in.readLine(out)) > 0) {
    ++lineNum;
    totalBytes+=numBytes;
  }
  in.close();
  assertEquals("total uncompressed bytes in concatenated test file",5346,totalBytes);
  assertEquals("total uncompressed lines in concatenated test file",84,lineNum);
  doMultipleGzipBufferSizes(jobConf,false);
  doMultipleGzipBufferSizes(jobConf,true);
}
