{
  setPhase(TaskStatus.Phase.CLEANUP);
  getProgress().setStatus("cleanup");
  statusUpdate(umbilical);
  LOG.info("Cleaning up job");
  if (jobRunStateForCleanup == JobStatus.State.FAILED || jobRunStateForCleanup == JobStatus.State.KILLED) {
    LOG.info("Aborting job with runstate : " + jobRunStateForCleanup.name());
    if (conf.getUseNewMapper()) {
      committer.abortJob(jobContext,jobRunStateForCleanup);
    }
 else {
      org.apache.hadoop.mapred.OutputCommitter oldCommitter=(org.apache.hadoop.mapred.OutputCommitter)committer;
      oldCommitter.abortJob(jobContext,jobRunStateForCleanup);
    }
  }
 else   if (jobRunStateForCleanup == JobStatus.State.SUCCEEDED) {
    LOG.info("Committing job");
    committer.commitJob(jobContext);
  }
 else {
    throw new IOException("Invalid state of the job for cleanup. State found " + jobRunStateForCleanup + " expecting "+ JobStatus.State.SUCCEEDED+ ", "+ JobStatus.State.FAILED+ " or "+ JobStatus.State.KILLED);
  }
  JobConf conf=new JobConf(jobContext.getConfiguration());
  if (!keepTaskFiles(conf)) {
    String jobTempDir=conf.get("mapreduce.job.dir");
    Path jobTempDirPath=new Path(jobTempDir);
    FileSystem fs=jobTempDirPath.getFileSystem(conf);
    fs.delete(jobTempDirPath,true);
  }
  done(umbilical,reporter);
}
