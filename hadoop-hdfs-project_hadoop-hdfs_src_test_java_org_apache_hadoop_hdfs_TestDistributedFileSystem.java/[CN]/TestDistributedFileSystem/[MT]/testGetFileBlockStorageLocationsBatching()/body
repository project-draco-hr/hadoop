{
  final Configuration conf=getTestConfiguration();
  conf.setBoolean(DFSConfigKeys.DFS_HDFS_BLOCKS_METADATA_ENABLED,true);
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
  DistributedFileSystem fs=cluster.getFileSystem();
  Path tmpFile1=new Path("/tmpfile1.dat");
  Path tmpFile2=new Path("/tmpfile2.dat");
  DFSTestUtil.createFile(fs,tmpFile1,1024,(short)2,0xDEADDEADl);
  DFSTestUtil.createFile(fs,tmpFile2,1024,(short)2,0xDEADDEADl);
  BlockLocation[] blockLocs1=fs.getFileBlockLocations(tmpFile1,0,1024);
  BlockLocation[] blockLocs2=fs.getFileBlockLocations(tmpFile2,0,1024);
  BlockLocation[] blockLocs=(BlockLocation[])ArrayUtils.addAll(blockLocs1,blockLocs2);
  BlockStorageLocation[] locs=fs.getFileBlockStorageLocations(Arrays.asList(blockLocs));
  int counter=0;
  for (  BlockStorageLocation l : locs) {
    for (int i=0; i < l.getVolumeIds().length; i++) {
      VolumeId id=l.getVolumeIds()[i];
      String name=l.getNames()[i];
      if (id != null) {
        System.out.println("Datanode " + name + " has block "+ counter+ " on volume id "+ id.toString());
      }
    }
    counter++;
  }
  assertEquals("Expected two HdfsBlockLocations for two 1-block files",2,locs.length);
  for (  BlockStorageLocation l : locs) {
    assertEquals("Expected two replicas for each block",2,l.getVolumeIds().length);
    for (int i=0; i < l.getVolumeIds().length; i++) {
      VolumeId id=l.getVolumeIds()[i];
      String name=l.getNames()[i];
      assertTrue("Expected block to be valid on datanode " + name,id.isValid());
    }
  }
}
