{
  JobConf job=new JobConf(conf);
  job.setBoolean("dfs.replication.considerLoad",false);
  dfs=new MiniDFSCluster.Builder(job).racks(rack1).hosts(hosts1).build();
  dfs.waitActive();
  String namenode=(dfs.getFileSystem()).getUri().getHost() + ":" + (dfs.getFileSystem()).getUri().getPort();
  FileSystem fileSys=dfs.getFileSystem();
  if (!fileSys.mkdirs(dir1)) {
    throw new IOException("Mkdirs failed to create " + root.toString());
  }
  writeFile(job,file1,(short)1,1);
  writeFile(job,file2,(short)1,1);
  DummyFileInputFormat inFormat=new DummyFileInputFormat();
  inFormat.setInputPaths(job,root);
  boolean exceptionThrown=false;
  try {
    InputSplit[] splits=inFormat.getSplits(job,1);
  }
 catch (  Exception e) {
    exceptionThrown=true;
  }
  assertTrue("Exception should be thrown by default for scanning a " + "directory with directories inside.",exceptionThrown);
  job.setBoolean(FileInputFormat.INPUT_DIR_RECURSIVE,true);
  InputSplit[] splits=inFormat.getSplits(job,1);
  assertEquals(splits.length,2);
}
