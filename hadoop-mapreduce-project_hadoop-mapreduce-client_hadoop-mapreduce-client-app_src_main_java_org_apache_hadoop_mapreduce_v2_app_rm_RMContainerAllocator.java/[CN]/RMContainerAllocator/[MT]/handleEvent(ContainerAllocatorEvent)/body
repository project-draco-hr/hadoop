{
  recalculateReduceSchedule=true;
  if (event.getType() == ContainerAllocator.EventType.CONTAINER_REQ) {
    ContainerRequestEvent reqEvent=(ContainerRequestEvent)event;
    JobId jobId=getJob().getID();
    Resource supportedMaxContainerCapability=getMaxContainerCapability();
    if (reqEvent.getAttemptID().getTaskId().getTaskType().equals(TaskType.MAP)) {
      if (mapResourceRequest.equals(Resources.none())) {
        mapResourceRequest=reqEvent.getCapability();
        eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.MAP,mapResourceRequest.getMemorySize())));
        LOG.info("mapResourceRequest:" + mapResourceRequest);
        if (mapResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || mapResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {
          String diagMsg="MAP capability required is more than the supported " + "max container capability in the cluster. Killing the Job. mapResourceRequest: " + mapResourceRequest + " maxContainerCapability:"+ supportedMaxContainerCapability;
          LOG.info(diagMsg);
          eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId,diagMsg));
          eventHandler.handle(new JobEvent(jobId,JobEventType.JOB_KILL));
        }
      }
      reqEvent.getCapability().setMemory(mapResourceRequest.getMemorySize());
      reqEvent.getCapability().setVirtualCores(mapResourceRequest.getVirtualCores());
      scheduledRequests.addMap(reqEvent);
    }
 else {
      if (reduceResourceRequest.equals(Resources.none())) {
        reduceResourceRequest=reqEvent.getCapability();
        eventHandler.handle(new JobHistoryEvent(jobId,new NormalizedResourceEvent(org.apache.hadoop.mapreduce.TaskType.REDUCE,reduceResourceRequest.getMemorySize())));
        LOG.info("reduceResourceRequest:" + reduceResourceRequest);
        if (reduceResourceRequest.getMemorySize() > supportedMaxContainerCapability.getMemorySize() || reduceResourceRequest.getVirtualCores() > supportedMaxContainerCapability.getVirtualCores()) {
          String diagMsg="REDUCE capability required is more than the " + "supported max container capability in the cluster. Killing the " + "Job. reduceResourceRequest: " + reduceResourceRequest + " maxContainerCapability:"+ supportedMaxContainerCapability;
          LOG.info(diagMsg);
          eventHandler.handle(new JobDiagnosticsUpdateEvent(jobId,diagMsg));
          eventHandler.handle(new JobEvent(jobId,JobEventType.JOB_KILL));
        }
      }
      reqEvent.getCapability().setMemory(reduceResourceRequest.getMemorySize());
      reqEvent.getCapability().setVirtualCores(reduceResourceRequest.getVirtualCores());
      if (reqEvent.getEarlierAttemptFailed()) {
        pendingReduces.addFirst(new ContainerRequest(reqEvent,PRIORITY_REDUCE,reduceNodeLabelExpression));
      }
 else {
        pendingReduces.add(new ContainerRequest(reqEvent,PRIORITY_REDUCE,reduceNodeLabelExpression));
      }
    }
  }
 else   if (event.getType() == ContainerAllocator.EventType.CONTAINER_DEALLOCATE) {
    LOG.info("Processing the event " + event.toString());
    TaskAttemptId aId=event.getAttemptID();
    boolean removed=scheduledRequests.remove(aId);
    if (!removed) {
      ContainerId containerId=assignedRequests.get(aId);
      if (containerId != null) {
        removed=true;
        assignedRequests.remove(aId);
        containersReleased++;
        pendingRelease.add(containerId);
        release(containerId);
      }
    }
    if (!removed) {
      LOG.error("Could not deallocate container for task attemptId " + aId);
    }
    preemptionPolicy.handleCompletedContainer(event.getAttemptID());
  }
 else   if (event.getType() == ContainerAllocator.EventType.CONTAINER_FAILED) {
    ContainerFailedEvent fEv=(ContainerFailedEvent)event;
    String host=getHost(fEv.getContMgrAddress());
    containerFailedOnHost(host);
    preemptionPolicy.handleFailedContainer(event.getAttemptID());
  }
}
