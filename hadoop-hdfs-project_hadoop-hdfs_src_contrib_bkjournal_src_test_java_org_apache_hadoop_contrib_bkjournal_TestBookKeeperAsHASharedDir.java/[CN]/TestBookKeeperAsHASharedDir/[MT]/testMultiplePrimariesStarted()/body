{
  Runtime mockRuntime1=mock(Runtime.class);
  Runtime mockRuntime2=mock(Runtime.class);
  Path p1=new Path("/testBKJMMultiplePrimary");
  MiniDFSCluster cluster=null;
  try {
    Configuration conf=new Configuration();
    conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY,1);
    conf.set(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY,BKJMUtil.createJournalURI("/hotfailoverMultiple").toString());
    BKJMUtil.addJournalManagerDefinition(conf);
    cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(0).manageNameDfsSharedDirs(false).build();
    NameNode nn1=cluster.getNameNode(0);
    NameNode nn2=cluster.getNameNode(1);
    FSEditLogTestUtil.setRuntimeForEditLog(nn1,mockRuntime1);
    FSEditLogTestUtil.setRuntimeForEditLog(nn2,mockRuntime2);
    cluster.waitActive();
    cluster.transitionToActive(0);
    FileSystem fs=HATestUtil.configureFailoverFs(cluster,conf);
    fs.mkdirs(p1);
    nn1.getRpcServer().rollEditLog();
    try {
      cluster.transitionToActive(1);
      fail("Shouldn't have been able to start two primaries" + " with single shared storage");
    }
 catch (    ServiceFailedException sfe) {
      assertTrue("Wrong exception",sfe.getMessage().contains("Failed to start active services"));
    }
  }
  finally {
    verify(mockRuntime1,times(0)).exit(anyInt());
    verify(mockRuntime2,atLeastOnce()).exit(anyInt());
    if (cluster != null) {
      cluster.shutdown();
    }
  }
}
