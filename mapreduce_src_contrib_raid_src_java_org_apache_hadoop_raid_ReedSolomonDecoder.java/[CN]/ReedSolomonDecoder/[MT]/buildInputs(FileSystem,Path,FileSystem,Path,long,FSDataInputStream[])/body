{
  LOG.info("Building inputs to recover block starting at " + errorOffset);
  FileStatus srcStat=fs.getFileStatus(srcFile);
  long blockSize=srcStat.getBlockSize();
  long blockIdx=(int)(errorOffset / blockSize);
  long stripeIdx=blockIdx / stripeSize;
  LOG.info("FileSize = " + srcStat.getLen() + ", blockSize = "+ blockSize+ ", blockIdx = "+ blockIdx+ ", stripeIdx = "+ stripeIdx);
  ArrayList<Integer> erasedLocations=new ArrayList<Integer>();
  for (int i=0; i < paritySize; i++) {
    long offset=blockSize * (stripeIdx * paritySize + i);
    FSDataInputStream in=parityFs.open(parityFile,conf.getInt("io.file.buffer.size",64 * 1024));
    in.seek(offset);
    LOG.info("Adding " + parityFile + ":"+ offset+ " as input "+ i);
    inputs[i]=in;
  }
  for (int i=paritySize; i < paritySize + stripeSize; i++) {
    long offset=blockSize * (stripeIdx * stripeSize + i - paritySize);
    if (offset == errorOffset) {
      LOG.info(srcFile + ":" + offset+ " is known to have error, adding zeros as input "+ i);
      inputs[i]=new FSDataInputStream(new RaidUtils.ZeroInputStream(offset + blockSize));
      erasedLocations.add(i);
    }
 else     if (offset > srcStat.getLen()) {
      LOG.info(srcFile + ":" + offset+ " is past file size, adding zeros as input "+ i);
      inputs[i]=new FSDataInputStream(new RaidUtils.ZeroInputStream(offset + blockSize));
    }
 else {
      FSDataInputStream in=fs.open(srcFile,conf.getInt("io.file.buffer.size",64 * 1024));
      in.seek(offset);
      LOG.info("Adding " + srcFile + ":"+ offset+ " as input "+ i);
      inputs[i]=in;
    }
  }
  if (erasedLocations.size() > paritySize) {
    String msg="Too many erased locations: " + erasedLocations.size();
    LOG.error(msg);
    throw new IOException(msg);
  }
  int[] locs=new int[erasedLocations.size()];
  for (int i=0; i < locs.length; i++) {
    locs[i]=erasedLocations.get(i);
  }
  return locs;
}
