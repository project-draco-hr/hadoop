{
  final Configuration conf=new HdfsConfiguration();
  int blockSize=5 * 1024 * 1024;
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
  conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
  conf.setLong(DFSConfigKeys.DFS_NAMENODE_REPLICATION_INTERVAL_KEY,1L);
  conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
  int numOfDatanodes=2;
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).racks(new String[]{"/default/rack0","/default/rack0"}).storagesPerDatanode(2).storageTypes(new StorageType[][]{{StorageType.SSD,StorageType.DISK},{StorageType.SSD,StorageType.DISK}}).storageCapacities(new long[][]{{100 * blockSize,20 * blockSize},{20 * blockSize,100 * blockSize}}).build();
  try {
    cluster.waitActive();
    DistributedFileSystem fs=cluster.getFileSystem();
    Path barDir=new Path("/bar");
    fs.mkdir(barDir,new FsPermission((short)777));
    fs.setStoragePolicy(barDir,HdfsConstants.ONESSD_STORAGE_POLICY_NAME);
    long fileLen=30 * blockSize;
    Path fooFile=new Path(barDir,"foo");
    createFile(cluster,fooFile,fileLen,(short)numOfDatanodes,0);
    cluster.triggerHeartbeats();
    BalancerParameters p=BalancerParameters.DEFAULT;
    Collection<URI> namenodes=DFSUtil.getNsServiceRpcUris(conf);
    final int r=Balancer.run(namenodes,p,conf);
    assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),r);
  }
  finally {
    cluster.shutdown();
  }
}
