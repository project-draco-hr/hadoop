{
  HdfsFileStatus stat=null;
  boolean shouldRetry=true;
  int retryCount=CREATE_RETRY_COUNT;
  while (shouldRetry) {
    shouldRetry=false;
    try {
      stat=dfsClient.namenode.create(src,masked,dfsClient.clientName,new EnumSetWritable<CreateFlag>(flag),createParent,replication,blockSize,cipherSuites);
      break;
    }
 catch (    RemoteException re) {
      IOException e=re.unwrapRemoteException(AccessControlException.class,DSQuotaExceededException.class,FileAlreadyExistsException.class,FileNotFoundException.class,ParentNotDirectoryException.class,NSQuotaExceededException.class,RetryStartFileException.class,SafeModeException.class,UnresolvedPathException.class,SnapshotAccessControlException.class,UnknownCipherSuiteException.class);
      if (e instanceof RetryStartFileException) {
        if (retryCount > 0) {
          shouldRetry=true;
          retryCount--;
        }
 else {
          throw new IOException("Too many retries because of encryption" + " zone operations",e);
        }
      }
 else {
        throw e;
      }
    }
  }
  Preconditions.checkNotNull(stat,"HdfsFileStatus should not be null!");
  final DFSOutputStream out=new DFSOutputStream(dfsClient,src,stat,flag,progress,checksum,favoredNodes);
  out.start();
  return out;
}
