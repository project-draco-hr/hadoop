{
  int shufflingReduceTasks=0;
  for (  TaskId taskId : job.reduceTasks) {
    Task task=job.tasks.get(taskId);
    if (TaskState.RUNNING.equals(task.getState())) {
      for (      TaskAttempt attempt : task.getAttempts().values()) {
        if (attempt.getPhase() == Phase.SHUFFLE) {
          shufflingReduceTasks++;
          break;
        }
      }
    }
  }
  JobTaskAttemptFetchFailureEvent fetchfailureEvent=(JobTaskAttemptFetchFailureEvent)event;
  for (  org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId mapId : fetchfailureEvent.getMaps()) {
    Integer fetchFailures=job.fetchFailuresMapping.get(mapId);
    fetchFailures=(fetchFailures == null) ? 1 : (fetchFailures + 1);
    job.fetchFailuresMapping.put(mapId,fetchFailures);
    float failureRate=shufflingReduceTasks == 0 ? 1.0f : (float)fetchFailures / shufflingReduceTasks;
    boolean isMapFaulty=(failureRate >= MAX_ALLOWED_FETCH_FAILURES_FRACTION);
    if (fetchFailures >= MAX_FETCH_FAILURES_NOTIFICATIONS && isMapFaulty) {
      LOG.info("Too many fetch-failures for output of task attempt: " + mapId + " ... raising fetch failure to map");
      job.eventHandler.handle(new TaskAttemptEvent(mapId,TaskAttemptEventType.TA_TOO_MANY_FETCH_FAILURE));
      job.fetchFailuresMapping.remove(mapId);
    }
  }
}
