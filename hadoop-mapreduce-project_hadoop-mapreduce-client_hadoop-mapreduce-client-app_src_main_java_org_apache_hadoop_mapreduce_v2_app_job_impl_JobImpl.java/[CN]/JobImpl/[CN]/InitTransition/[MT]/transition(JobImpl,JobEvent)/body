{
  job.metrics.submittedJob(job);
  job.metrics.preparingJob(job);
  try {
    setup(job);
    job.fs=job.getFileSystem(job.conf);
    JobSubmittedEvent jse=new JobSubmittedEvent(job.oldJobId,job.conf.get(MRJobConfig.JOB_NAME,"test"),job.conf.get(MRJobConfig.USER_NAME,"mapred"),job.appSubmitTime,job.remoteJobConfFile.toString(),job.jobACLs,job.conf.get(MRJobConfig.QUEUE_NAME,"default"));
    job.eventHandler.handle(new JobHistoryEvent(job.jobId,jse));
    TaskSplitMetaInfo[] taskSplitMetaInfo=createSplits(job,job.jobId);
    job.numMapTasks=taskSplitMetaInfo.length;
    job.numReduceTasks=job.conf.getInt(MRJobConfig.NUM_REDUCES,0);
    if (job.numMapTasks == 0 && job.numReduceTasks == 0) {
      job.addDiagnostic("No of maps and reduces are 0 " + job.jobId);
    }
    checkTaskLimits();
    if (job.newApiCommitter) {
      job.jobContext=new JobContextImpl(job.conf,job.oldJobId);
    }
 else {
      job.jobContext=new org.apache.hadoop.mapred.JobContextImpl(new JobConf(job.conf),job.oldJobId);
    }
    long inputLength=0;
    for (int i=0; i < job.numMapTasks; ++i) {
      inputLength+=taskSplitMetaInfo[i].getInputDataLength();
    }
    int sysMaxMaps=job.conf.getInt(MRJobConfig.JOB_UBERTASK_MAXMAPS,9);
    int sysMaxReduces=job.conf.getInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES,1);
    long sysMaxBytes=job.conf.getLong(MRJobConfig.JOB_UBERTASK_MAXBYTES,job.conf.getLong("dfs.block.size",64 * 1024 * 1024));
    boolean uberEnabled=job.conf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE,false);
    boolean smallNumMapTasks=(job.numMapTasks <= sysMaxMaps);
    boolean smallNumReduceTasks=(job.numReduceTasks <= sysMaxReduces);
    boolean smallInput=(inputLength <= sysMaxBytes);
    boolean smallMemory=true;
    boolean notChainJob=!isChainJob(job.conf);
    job.isUber=uberEnabled && smallNumMapTasks && smallNumReduceTasks&& smallInput&& smallMemory&& notChainJob;
    if (job.isUber) {
      LOG.info("Uberizing job " + job.jobId + ": "+ job.numMapTasks+ "m+"+ job.numReduceTasks+ "r tasks ("+ inputLength+ " input bytes) will run sequentially on single node.");
      job.conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART,1.0f);
      job.conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS,1);
      job.conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS,1);
    }
 else {
      StringBuilder msg=new StringBuilder();
      msg.append("Not uberizing ").append(job.jobId).append(" because:");
      if (!uberEnabled)       msg.append(" not enabled;");
      if (!smallNumMapTasks)       msg.append(" too many maps;");
      if (!smallNumReduceTasks)       msg.append(" too many reduces;");
      if (!smallInput)       msg.append(" too much input;");
      if (!smallMemory)       msg.append(" too much RAM;");
      if (!notChainJob)       msg.append(" chainjob");
      LOG.info(msg.toString());
    }
    job.taskAttemptCompletionEvents=new ArrayList<TaskAttemptCompletionEvent>(job.numMapTasks + job.numReduceTasks + 10);
    job.allowedMapFailuresPercent=job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT,0);
    job.allowedReduceFailuresPercent=job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT,0);
    job.committer.setupJob(job.jobContext);
    job.setupProgress=1.0f;
    createMapTasks(job,inputLength,taskSplitMetaInfo);
    createReduceTasks(job);
    job.metrics.endPreparingJob(job);
    return JobState.INITED;
  }
 catch (  IOException e) {
    LOG.warn("Job init failed",e);
    job.addDiagnostic("Job init failed : " + StringUtils.stringifyException(e));
    job.abortJob(org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    job.metrics.endPreparingJob(job);
    return job.finished(JobState.FAILED);
  }
}
