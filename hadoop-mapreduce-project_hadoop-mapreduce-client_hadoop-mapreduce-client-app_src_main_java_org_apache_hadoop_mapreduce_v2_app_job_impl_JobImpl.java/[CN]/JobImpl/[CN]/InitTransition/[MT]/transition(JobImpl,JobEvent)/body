{
  job.metrics.submittedJob(job);
  job.metrics.preparingJob(job);
  try {
    setup(job);
    job.fs=job.getFileSystem(job.conf);
    JobSubmittedEvent jse=new JobSubmittedEvent(job.oldJobId,job.conf.get(MRJobConfig.JOB_NAME,"test"),job.conf.get(MRJobConfig.USER_NAME,"mapred"),job.appSubmitTime,job.remoteJobConfFile.toString(),job.jobACLs,job.queueName);
    job.eventHandler.handle(new JobHistoryEvent(job.jobId,jse));
    TaskSplitMetaInfo[] taskSplitMetaInfo=createSplits(job,job.jobId);
    job.numMapTasks=taskSplitMetaInfo.length;
    job.numReduceTasks=job.conf.getInt(MRJobConfig.NUM_REDUCES,0);
    if (job.numMapTasks == 0 && job.numReduceTasks == 0) {
      job.addDiagnostic("No of maps and reduces are 0 " + job.jobId);
    }
 else     if (job.numMapTasks == 0) {
      job.reduceWeight=0.9f;
    }
 else     if (job.numReduceTasks == 0) {
      job.mapWeight=0.9f;
    }
 else {
      job.mapWeight=job.reduceWeight=0.45f;
    }
    checkTaskLimits();
    if (job.newApiCommitter) {
      job.jobContext=new JobContextImpl(job.conf,job.oldJobId);
    }
 else {
      job.jobContext=new org.apache.hadoop.mapred.JobContextImpl(job.conf,job.oldJobId);
    }
    long inputLength=0;
    for (int i=0; i < job.numMapTasks; ++i) {
      inputLength+=taskSplitMetaInfo[i].getInputDataLength();
    }
    job.makeUberDecision(inputLength);
    job.taskAttemptCompletionEvents=new ArrayList<TaskAttemptCompletionEvent>(job.numMapTasks + job.numReduceTasks + 10);
    job.mapAttemptCompletionEvents=new ArrayList<TaskCompletionEvent>(job.numMapTasks + 10);
    job.taskCompletionIdxToMapCompletionIdx=new ArrayList<Integer>(job.numMapTasks + job.numReduceTasks + 10);
    job.allowedMapFailuresPercent=job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT,0);
    job.allowedReduceFailuresPercent=job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT,0);
    createMapTasks(job,inputLength,taskSplitMetaInfo);
    createReduceTasks(job);
    job.metrics.endPreparingJob(job);
    return JobStateInternal.INITED;
  }
 catch (  IOException e) {
    LOG.warn("Job init failed",e);
    job.metrics.endPreparingJob(job);
    job.addDiagnostic("Job init failed : " + StringUtils.stringifyException(e));
    job.eventHandler.handle(new CommitterJobAbortEvent(job.jobId,job.jobContext,org.apache.hadoop.mapreduce.JobStatus.State.FAILED));
    return JobStateInternal.FAILED;
  }
}
