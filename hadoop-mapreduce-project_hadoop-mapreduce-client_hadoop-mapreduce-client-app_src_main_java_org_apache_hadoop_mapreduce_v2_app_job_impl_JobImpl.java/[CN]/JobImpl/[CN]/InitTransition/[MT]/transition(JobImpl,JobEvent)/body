{
  job.submitTime=job.clock.getTime();
  job.metrics.submittedJob(job);
  job.metrics.preparingJob(job);
  try {
    setup(job);
    job.fs=job.getFileSystem(job.conf);
    JobSubmittedEvent jse=new JobSubmittedEvent(job.oldJobId,job.conf.get(MRJobConfig.JOB_NAME,"test"),job.conf.get(MRJobConfig.USER_NAME,"mapred"),job.submitTime,job.remoteJobConfFile.toString(),job.jobACLs,job.conf.get(MRJobConfig.QUEUE_NAME,"default"));
    job.eventHandler.handle(new JobHistoryEvent(job.jobId,jse));
    TaskSplitMetaInfo[] taskSplitMetaInfo=createSplits(job,job.jobId);
    job.numMapTasks=taskSplitMetaInfo.length;
    job.numReduceTasks=job.conf.getInt(MRJobConfig.NUM_REDUCES,0);
    if (job.numMapTasks == 0 && job.numReduceTasks == 0) {
      job.addDiagnostic("No of maps and reduces are 0 " + job.jobId);
    }
    checkTaskLimits();
    boolean newApiCommitter=false;
    if ((job.numReduceTasks > 0 && job.conf.getBoolean("mapred.reducer.new-api",false)) || (job.numReduceTasks == 0 && job.conf.getBoolean("mapred.mapper.new-api",false))) {
      newApiCommitter=true;
      LOG.info("Using mapred newApiCommitter.");
    }
    LOG.info("OutputCommitter set in config " + job.conf.get("mapred.output.committer.class"));
    if (newApiCommitter) {
      job.jobContext=new JobContextImpl(job.conf,job.oldJobId);
      org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId attemptID=RecordFactoryProvider.getRecordFactory(null).newRecordInstance(org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId.class);
      attemptID.setTaskId(RecordFactoryProvider.getRecordFactory(null).newRecordInstance(TaskId.class));
      attemptID.getTaskId().setJobId(job.jobId);
      attemptID.getTaskId().setTaskType(TaskType.MAP);
      TaskAttemptContext taskContext=new TaskAttemptContextImpl(job.conf,TypeConverter.fromYarn(attemptID));
      try {
        OutputFormat outputFormat=ReflectionUtils.newInstance(taskContext.getOutputFormatClass(),job.conf);
        job.committer=outputFormat.getOutputCommitter(taskContext);
      }
 catch (      Exception e) {
        throw new IOException("Failed to assign outputcommitter",e);
      }
    }
 else {
      job.jobContext=new org.apache.hadoop.mapred.JobContextImpl(new JobConf(job.conf),job.oldJobId);
      job.committer=ReflectionUtils.newInstance(job.conf.getClass("mapred.output.committer.class",FileOutputCommitter.class,org.apache.hadoop.mapred.OutputCommitter.class),job.conf);
    }
    LOG.info("OutputCommitter is " + job.committer.getClass().getName());
    long inputLength=0;
    for (int i=0; i < job.numMapTasks; ++i) {
      inputLength+=taskSplitMetaInfo[i].getInputDataLength();
    }
    int sysMaxMaps=job.conf.getInt(MRJobConfig.JOB_UBERTASK_MAXMAPS,9);
    int sysMaxReduces=job.conf.getInt(MRJobConfig.JOB_UBERTASK_MAXREDUCES,1);
    long sysMaxBytes=job.conf.getLong(MRJobConfig.JOB_UBERTASK_MAXBYTES,job.conf.getLong("dfs.block.size",64 * 1024 * 1024));
    boolean uberEnabled=job.conf.getBoolean(MRJobConfig.JOB_UBERTASK_ENABLE,false);
    boolean smallNumMapTasks=(job.numMapTasks <= sysMaxMaps);
    boolean smallNumReduceTasks=(job.numReduceTasks <= sysMaxReduces);
    boolean smallInput=(inputLength <= sysMaxBytes);
    boolean smallMemory=true;
    boolean notChainJob=!isChainJob(job.conf);
    job.isUber=uberEnabled && smallNumMapTasks && smallNumReduceTasks&& smallInput&& smallMemory&& notChainJob;
    if (job.isUber) {
      LOG.info("Uberizing job " + job.jobId + ": "+ job.numMapTasks+ "m+"+ job.numReduceTasks+ "r tasks ("+ inputLength+ " input bytes) will run sequentially on single node.");
      job.conf.setFloat(MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART,1.0f);
      job.conf.setInt(MRJobConfig.MAP_MAX_ATTEMPTS,1);
      job.conf.setInt(MRJobConfig.REDUCE_MAX_ATTEMPTS,1);
    }
 else {
      StringBuilder msg=new StringBuilder();
      msg.append("Not uberizing ").append(job.jobId).append(" because:");
      if (!uberEnabled)       msg.append(" not enabled;");
      if (!smallNumMapTasks)       msg.append(" too many maps;");
      if (!smallNumReduceTasks)       msg.append(" too many reduces;");
      if (!smallInput)       msg.append(" too much input;");
      if (!smallMemory)       msg.append(" too much RAM;");
      if (!notChainJob)       msg.append(" chainjob");
      LOG.info(msg.toString());
    }
    job.taskAttemptCompletionEvents=new ArrayList<TaskAttemptCompletionEvent>(job.numMapTasks + job.numReduceTasks + 10);
    job.allowedMapFailuresPercent=job.conf.getInt(MRJobConfig.MAP_FAILURES_MAX_PERCENT,0);
    job.allowedReduceFailuresPercent=job.conf.getInt(MRJobConfig.REDUCE_FAILURES_MAXPERCENT,0);
    job.committer.setupJob(job.jobContext);
    job.setupProgress=1.0f;
    createMapTasks(job,inputLength,taskSplitMetaInfo);
    createReduceTasks(job);
    job.metrics.endPreparingJob(job);
    return JobState.INITED;
  }
 catch (  IOException e) {
    LOG.warn("Job init failed",e);
    job.addDiagnostic("Job init failed : " + StringUtils.stringifyException(e));
    job.abortJob(org.apache.hadoop.mapreduce.JobStatus.State.FAILED);
    job.metrics.endPreparingJob(job);
    return job.finished(JobState.FAILED);
  }
}
