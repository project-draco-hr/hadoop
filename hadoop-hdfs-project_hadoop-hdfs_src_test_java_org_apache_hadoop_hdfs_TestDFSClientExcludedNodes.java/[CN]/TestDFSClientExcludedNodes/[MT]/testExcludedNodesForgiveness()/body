{
  Configuration conf=new HdfsConfiguration();
  conf.setLong(DFSConfigKeys.DFS_CLIENT_WRITE_EXCLUDE_NODES_CACHE_EXPIRY_INTERVAL,1000);
  conf.setInt("io.bytes.per.checksum",512);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
  List<DataNodeProperties> props=cluster.dataNodes;
  FileSystem fs=cluster.getFileSystem();
  Path filePath=new Path("/testForgivingExcludedNodes");
  byte[] bytes=new byte[256];
  for (int index=0; index < bytes.length; index++) {
    bytes[index]='0';
  }
  FSDataOutputStream out=fs.create(filePath,true,4096,(short)3,512);
  out.write(bytes);
  out.write(bytes);
  out.hflush();
  DataNodeProperties two=cluster.stopDataNode(2);
  DataNodeProperties one=cluster.stopDataNode(1);
  out.write(bytes);
  out.write(bytes);
  out.hflush();
  Assert.assertEquals(true,cluster.restartDataNode(one,true));
  Assert.assertEquals(true,cluster.restartDataNode(two,true));
  cluster.waitActive();
  ThreadUtil.sleepAtLeastIgnoreInterrupts(2000);
  cluster.stopDataNode(0);
  try {
    out.write(bytes);
    out.hflush();
    out.close();
  }
 catch (  Exception e) {
    fail("Excluded DataNodes should be forgiven after a while and " + "not cause file writing exception of: '" + e.getMessage() + "'");
  }
}
