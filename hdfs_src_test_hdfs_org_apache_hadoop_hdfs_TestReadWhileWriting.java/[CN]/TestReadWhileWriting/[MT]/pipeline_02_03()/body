{
  final Configuration conf=new HdfsConfiguration();
  conf.setBoolean(DFSConfigKeys.DFS_SUPPORT_APPEND_KEY,true);
  conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(4).build();
  try {
    cluster.setLeasePeriod(SOFT_LEASE_LIMIT,HARD_LEASE_LIMIT);
    cluster.waitActive();
    final FileSystem fs=cluster.getFileSystem();
    final Path p=new Path(DIR,"file1");
    final int half=BLOCK_SIZE / 2;
{
      final FSDataOutputStream out=fs.create(p,true,fs.getConf().getInt("io.file.buffer.size",4096),(short)3,BLOCK_SIZE);
      write(out,0,half);
      ((DFSOutputStream)out.getWrappedStream()).hflush();
    }
    checkFile(p,half,conf);
    AppendTestUtil.LOG.info("leasechecker.interruptAndJoin()");
    ((DistributedFileSystem)fs).dfs.leaserenewer.interruptAndJoin();
{
      Thread.sleep(2 * SOFT_LEASE_LIMIT);
      final UserGroupInformation current=UserGroupInformation.getCurrentUser();
      final UserGroupInformation ugi=UserGroupInformation.createUserForTesting(current.getShortUserName() + "x",new String[]{"supergroup"});
      final DistributedFileSystem dfs=ugi.doAs(new PrivilegedExceptionAction<DistributedFileSystem>(){
        @Override public DistributedFileSystem run() throws Exception {
          return (DistributedFileSystem)FileSystem.newInstance(conf);
        }
      }
);
      final FSDataOutputStream out=append(dfs,p);
      write(out,0,half);
      out.close();
    }
    checkFile(p,2 * half,conf);
  }
  finally {
    cluster.shutdown();
  }
}
