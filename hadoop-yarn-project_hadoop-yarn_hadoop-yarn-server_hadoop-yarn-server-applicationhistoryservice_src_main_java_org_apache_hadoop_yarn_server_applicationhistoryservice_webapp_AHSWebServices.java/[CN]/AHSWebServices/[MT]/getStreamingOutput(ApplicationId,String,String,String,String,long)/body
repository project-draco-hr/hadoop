{
  String suffix=LogAggregationUtils.getRemoteNodeLogDirSuffix(conf);
  org.apache.hadoop.fs.Path remoteRootLogDir=new org.apache.hadoop.fs.Path(conf.get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));
  org.apache.hadoop.fs.Path qualifiedRemoteRootLogDir=FileContext.getFileContext(conf).makeQualified(remoteRootLogDir);
  FileContext fc=FileContext.getFileContext(qualifiedRemoteRootLogDir.toUri(),conf);
  org.apache.hadoop.fs.Path remoteAppDir=null;
  if (appOwner == null) {
    org.apache.hadoop.fs.Path toMatch=LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir,appId,"*",suffix);
    FileStatus[] matching=fc.util().globStatus(toMatch);
    if (matching == null || matching.length != 1) {
      return null;
    }
    remoteAppDir=matching[0].getPath();
  }
 else {
    remoteAppDir=LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir,appId,appOwner,suffix);
  }
  final RemoteIterator<FileStatus> nodeFiles;
  nodeFiles=fc.listStatus(remoteAppDir);
  if (!nodeFiles.hasNext()) {
    return null;
  }
  StreamingOutput stream=new StreamingOutput(){
    @Override public void write(    OutputStream os) throws IOException, WebApplicationException {
      byte[] buf=new byte[65535];
      boolean findLogs=false;
      while (nodeFiles.hasNext()) {
        final FileStatus thisNodeFile=nodeFiles.next();
        String nodeName=thisNodeFile.getPath().getName();
        if ((nodeId == null || nodeName.contains(LogAggregationUtils.getNodeString(nodeId))) && !nodeName.endsWith(LogAggregationUtils.TMP_FILE_SUFFIX)) {
          AggregatedLogFormat.LogReader reader=null;
          try {
            reader=new AggregatedLogFormat.LogReader(conf,thisNodeFile.getPath());
            DataInputStream valueStream;
            LogKey key=new LogKey();
            valueStream=reader.next(key);
            while (valueStream != null && !key.toString().equals(containerIdStr)) {
              key=new LogKey();
              valueStream=reader.next(key);
            }
            if (valueStream == null) {
              continue;
            }
            while (true) {
              try {
                String fileType=valueStream.readUTF();
                String fileLengthStr=valueStream.readUTF();
                long fileLength=Long.parseLong(fileLengthStr);
                if (fileType.equalsIgnoreCase(logFile)) {
                  StringBuilder sb=new StringBuilder();
                  sb.append("LogType:");
                  sb.append(fileType + "\n");
                  sb.append("Log Upload Time:");
                  sb.append(Times.format(System.currentTimeMillis()) + "\n");
                  sb.append("LogLength:");
                  sb.append(fileLengthStr + "\n");
                  sb.append("Log Contents:\n");
                  byte[] b=sb.toString().getBytes(Charset.forName("UTF-8"));
                  os.write(b,0,b.length);
                  long toSkip=0;
                  long totalBytesToRead=fileLength;
                  long skipAfterRead=0;
                  if (bytes < 0) {
                    long absBytes=Math.abs(bytes);
                    if (absBytes < fileLength) {
                      toSkip=fileLength - absBytes;
                      totalBytesToRead=absBytes;
                    }
                    org.apache.hadoop.io.IOUtils.skipFully(valueStream,toSkip);
                  }
 else {
                    if (bytes < fileLength) {
                      totalBytesToRead=bytes;
                      skipAfterRead=fileLength - bytes;
                    }
                  }
                  long curRead=0;
                  long pendingRead=totalBytesToRead - curRead;
                  int toRead=pendingRead > buf.length ? buf.length : (int)pendingRead;
                  int len=valueStream.read(buf,0,toRead);
                  while (len != -1 && curRead < totalBytesToRead) {
                    os.write(buf,0,len);
                    curRead+=len;
                    pendingRead=totalBytesToRead - curRead;
                    toRead=pendingRead > buf.length ? buf.length : (int)pendingRead;
                    len=valueStream.read(buf,0,toRead);
                  }
                  org.apache.hadoop.io.IOUtils.skipFully(valueStream,skipAfterRead);
                  sb=new StringBuilder();
                  sb.append("\nEnd of LogType:" + fileType + "\n");
                  b=sb.toString().getBytes(Charset.forName("UTF-8"));
                  os.write(b,0,b.length);
                  findLogs=true;
                }
 else {
                  long totalSkipped=0;
                  long currSkipped=0;
                  while (currSkipped != -1 && totalSkipped < fileLength) {
                    currSkipped=valueStream.skip(fileLength - totalSkipped);
                    totalSkipped+=currSkipped;
                  }
                }
              }
 catch (              EOFException eof) {
                break;
              }
            }
          }
  finally {
            if (reader != null) {
              reader.close();
            }
          }
        }
      }
      os.flush();
      if (!findLogs) {
        throw new IOException("Can not find logs for container:" + containerIdStr);
      }
    }
  }
;
  return stream;
}
