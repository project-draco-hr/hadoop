def _getDataNodeCommand(self, id):
    sd = self.serviceDesc
    parentDirs = self.workDirs
    workDirs = []
    attrs = sd.getfinalAttrs().copy()
    envs = sd.getEnvs().copy()
    nn = self.masterAddr
    if (nn == None):
        raise ValueError, "Can't get namenode address"
    attrs['fs.default.name'] = nn
    if (self.version < 16):
        if ('dfs.datanode.port' not in attrs):
            attrs['dfs.datanode.port'] = 'fillinport'
        if ('dfs.datanode.info.port' not in attrs):
            attrs['dfs.datanode.info.port'] = 'fillinport'
    else:
        if ('dfs.datanode.address' not in attrs):
            attrs['dfs.datanode.address'] = 'fillinhostport'
        if ('dfs.datanode.http.address' not in attrs):
            attrs['dfs.datanode.http.address'] = 'fillinhostport'
    if (self.version >= 18):
        attrs['dfs.datanode.ipc.address'] = 'fillinhostport'
    pd = []
    for dir in parentDirs:
        dir = ((dir + '-') + id)
        pd.append(dir)
    parentDirs = pd
    self._setWorkDirs(workDirs, envs, attrs, parentDirs, 'hdfs-dn')
    dict = {'name': 'datanode', }
    dict['program'] = os.path.join('bin', 'hadoop')
    dict['argv'] = ['datanode']
    dict['envs'] = envs
    dict['pkgdirs'] = sd.getPkgDirs()
    dict['workdirs'] = workDirs
    dict['final-attrs'] = attrs
    dict['attrs'] = sd.getAttrs()
    cmd = CommandDesc(dict)
    return cmd
