{
  if (conf == null)   conf=new HdfsConfiguration();
  StartupOption startOpt=parseArguments(argv);
  if (startOpt == null) {
    printUsage();
    return null;
  }
  setStartupOption(conf,startOpt);
  if (HAUtil.isHAEnabled(conf,DFSUtil.getNamenodeNameServiceId(conf)) && (startOpt == StartupOption.UPGRADE || startOpt == StartupOption.ROLLBACK || startOpt == StartupOption.FINALIZE)) {
    throw new HadoopIllegalArgumentException("Invalid startup option. " + "Cannot perform DFS upgrade with HA enabled.");
  }
switch (startOpt) {
case FORMAT:
{
      boolean aborted=format(conf,false);
      System.exit(aborted ? 1 : 0);
      return null;
    }
case GENCLUSTERID:
{
    System.err.println("Generating new cluster id:");
    System.out.println(NNStorage.newClusterID());
    System.exit(0);
    return null;
  }
case FINALIZE:
{
  boolean aborted=finalize(conf,true);
  System.exit(aborted ? 1 : 0);
  return null;
}
case BOOTSTRAPSTANDBY:
{
String toolArgs[]=Arrays.copyOfRange(argv,1,argv.length);
int rc=BootstrapStandby.run(toolArgs,conf);
System.exit(rc);
return null;
}
case INITIALIZESHAREDEDITS:
{
boolean aborted=initializeSharedEdits(conf,false,true);
System.exit(aborted ? 1 : 0);
return null;
}
case BACKUP:
case CHECKPOINT:
{
NamenodeRole role=startOpt.toNodeRole();
DefaultMetricsSystem.initialize(role.toString().replace(" ",""));
return new BackupNode(conf,role);
}
case RECOVER:
{
NameNode.doRecovery(startOpt,conf);
return null;
}
default :
DefaultMetricsSystem.initialize("NameNode");
return new NameNode(conf);
}
}
