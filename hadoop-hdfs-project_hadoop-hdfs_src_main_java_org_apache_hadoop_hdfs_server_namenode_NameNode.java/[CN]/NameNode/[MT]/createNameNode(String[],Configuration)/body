{
  if (conf == null)   conf=new HdfsConfiguration();
  StartupOption startOpt=parseArguments(argv);
  if (startOpt == null) {
    printUsage(System.err);
    return null;
  }
  setStartupOption(conf,startOpt);
  if (HAUtil.isHAEnabled(conf,DFSUtil.getNamenodeNameServiceId(conf)) && (startOpt == StartupOption.UPGRADE || startOpt == StartupOption.ROLLBACK || startOpt == StartupOption.FINALIZE)) {
    throw new HadoopIllegalArgumentException("Invalid startup option. " + "Cannot perform DFS upgrade with HA enabled.");
  }
switch (startOpt) {
case FORMAT:
{
      boolean aborted=format(conf,startOpt.getForceFormat(),startOpt.getInteractiveFormat());
      terminate(aborted ? 1 : 0);
      return null;
    }
case GENCLUSTERID:
{
    System.err.println("Generating new cluster id:");
    System.out.println(NNStorage.newClusterID());
    terminate(0);
    return null;
  }
case FINALIZE:
{
  boolean aborted=finalize(conf,true);
  terminate(aborted ? 1 : 0);
  return null;
}
case BOOTSTRAPSTANDBY:
{
String toolArgs[]=Arrays.copyOfRange(argv,1,argv.length);
int rc=BootstrapStandby.run(toolArgs,conf);
terminate(rc);
return null;
}
case INITIALIZESHAREDEDITS:
{
boolean aborted=initializeSharedEdits(conf,startOpt.getForceFormat(),startOpt.getInteractiveFormat());
terminate(aborted ? 1 : 0);
return null;
}
case BACKUP:
case CHECKPOINT:
{
NamenodeRole role=startOpt.toNodeRole();
DefaultMetricsSystem.initialize(role.toString().replace(" ",""));
return new BackupNode(conf,role);
}
case RECOVER:
{
NameNode.doRecovery(startOpt,conf);
return null;
}
default :
{
DefaultMetricsSystem.initialize("NameNode");
return new NameNode(conf);
}
}
}
