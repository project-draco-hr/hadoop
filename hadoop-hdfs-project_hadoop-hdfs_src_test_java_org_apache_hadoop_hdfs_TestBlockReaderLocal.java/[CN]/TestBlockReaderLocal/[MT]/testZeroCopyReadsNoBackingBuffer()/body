{
  HdfsConfiguration conf=initZeroCopyTest();
  MiniDFSCluster cluster=null;
  final Path TEST_PATH=new Path("/a");
  FSDataInputStream fsIn=null;
  ZeroCopyCursor zcursor=null;
  FileSystem fs=null;
  try {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,TEST_PATH,BlockReaderLocalTest.TEST_LENGTH,(short)1,7567L);
    try {
      DFSTestUtil.waitReplication(fs,TEST_PATH,(short)1);
    }
 catch (    InterruptedException e) {
      Assert.fail("unexpected InterruptedException during " + "waitReplication: " + e);
    }
catch (    TimeoutException e) {
      Assert.fail("unexpected TimeoutException during " + "waitReplication: " + e);
    }
    fsIn=fs.open(TEST_PATH);
    byte original[]=new byte[BlockReaderLocalTest.TEST_LENGTH];
    IOUtils.readFully(fsIn,original,0,BlockReaderLocalTest.TEST_LENGTH);
    fsIn.close();
    fsIn=fs.open(TEST_PATH);
    zcursor=fsIn.createZeroCopyCursor();
    zcursor.setAllowShortReads(false);
    HdfsDataInputStream dfsIn=(HdfsDataInputStream)fsIn;
    try {
      zcursor.read(8192);
      Assert.fail("expected UnsupportedOperationException");
    }
 catch (    UnsupportedOperationException e) {
    }
    try {
      zcursor.read(4097);
      Assert.fail("expected UnsupportedOperationException");
    }
 catch (    UnsupportedOperationException e) {
    }
    zcursor.read(4096);
    ByteBuffer result=zcursor.getData();
    Assert.assertEquals(4096,result.remaining());
    Assert.assertEquals(4096,dfsIn.getReadStatistics().getTotalBytesRead());
    Assert.assertEquals(4096,dfsIn.getReadStatistics().getTotalZeroCopyBytesRead());
    Assert.assertArrayEquals(Arrays.copyOfRange(original,0,4096),byteBufferToArray(result));
  }
  finally {
    if (zcursor != null)     zcursor.close();
    if (fsIn != null)     fsIn.close();
    if (fs != null)     fs.close();
    if (cluster != null)     cluster.shutdown();
  }
}
