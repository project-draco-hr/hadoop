{
  StringBuilder fileName=new StringBuilder("testFormat.txt");
  if (codec != null) {
    fileName.append(".gz");
  }
  localFs.delete(workDir,true);
  Path file=new Path(workDir,fileName.toString());
  int seed=new Random().nextInt();
  LOG.info("Seed = " + seed);
  Random random=new Random(seed);
  int MAX_TESTS=20;
  LongWritable key;
  BytesWritable value;
  for (int i=0; i < MAX_TESTS; i++) {
    LOG.info("----------------------------------------------------------");
    int totalRecords=random.nextInt(999) + 1;
    if (i == 8) {
      totalRecords=0;
    }
    int recordLength=random.nextInt(1024 * 100) + 1;
    if (i == 10) {
      recordLength=1;
    }
    int fileSize=(totalRecords * recordLength);
    LOG.info("totalRecords=" + totalRecords + " recordLength="+ recordLength);
    ArrayList<String> recordList=createFile(file,codec,recordLength,totalRecords);
    assertTrue(localFs.exists(file));
    Job job=Job.getInstance(defaultConf);
    FixedLengthInputFormat.setRecordLength(job.getConfiguration(),recordLength);
    int numSplits=1;
    if (i > 0) {
      if (i == (MAX_TESTS - 1)) {
        numSplits=(int)(fileSize / Math.floor(recordLength / 2));
      }
 else {
        if (MAX_TESTS % i == 0) {
          numSplits=fileSize / (fileSize - random.nextInt(fileSize));
        }
 else {
          numSplits=Math.max(1,fileSize / random.nextInt(Integer.MAX_VALUE));
        }
      }
      LOG.info("Number of splits set to: " + numSplits);
    }
    job.getConfiguration().setLong("mapreduce.input.fileinputformat.split.maxsize",(long)(fileSize / numSplits));
    FileInputFormat.setInputPaths(job,workDir);
    FixedLengthInputFormat format=new FixedLengthInputFormat();
    List<InputSplit> splits=format.getSplits(job);
    LOG.info("Actual number of splits = " + splits.size());
    long recordOffset=0;
    int recordNumber=0;
    for (    InputSplit split : splits) {
      TaskAttemptContext context=MapReduceTestUtil.createDummyMapTaskAttemptContext(job.getConfiguration());
      RecordReader<LongWritable,BytesWritable> reader=format.createRecordReader(split,context);
      MapContext<LongWritable,BytesWritable,LongWritable,BytesWritable> mcontext=new MapContextImpl<LongWritable,BytesWritable,LongWritable,BytesWritable>(job.getConfiguration(),context.getTaskAttemptID(),reader,null,null,MapReduceTestUtil.createDummyReporter(),split);
      reader.initialize(split,mcontext);
      Class<?> clazz=reader.getClass();
      assertEquals("RecordReader class should be FixedLengthRecordReader:",FixedLengthRecordReader.class,clazz);
      while (reader.nextKeyValue()) {
        key=reader.getCurrentKey();
        value=reader.getCurrentValue();
        assertEquals("Checking key",(long)(recordNumber * recordLength),key.get());
        String valueString=new String(value.getBytes(),0,value.getLength());
        assertEquals("Checking record length:",recordLength,value.getLength());
        assertTrue("Checking for more records than expected:",recordNumber < totalRecords);
        String origRecord=recordList.get(recordNumber);
        assertEquals("Checking record content:",origRecord,valueString);
        recordNumber++;
      }
      reader.close();
    }
    assertEquals("Total original records should be total read records:",recordList.size(),recordNumber);
  }
}
