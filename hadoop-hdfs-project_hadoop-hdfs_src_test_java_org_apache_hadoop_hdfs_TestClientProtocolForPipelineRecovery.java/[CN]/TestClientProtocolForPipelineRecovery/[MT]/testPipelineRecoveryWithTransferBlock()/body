{
  final int chunkSize=512;
  final int oneWriteSize=5000;
  final int totalSize=1024 * 1024;
  final int errorInjectionPos=512;
  Configuration conf=new HdfsConfiguration();
  final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(4).build();
  DataNodeFaultInjector old=DataNodeFaultInjector.get();
  try {
    DistributedFileSystem fs=cluster.getFileSystem();
    Path fileName=new Path("/f");
    FSDataOutputStream o=fs.create(fileName);
    int count=0;
    o.writeBytes("hello");
    o.hflush();
    DFSOutputStream dfsO=(DFSOutputStream)o.getWrappedStream();
    final DatanodeInfo[] pipeline=dfsO.getStreamer().getNodes();
    final String lastDn=pipeline[2].getXferAddr(false);
    final AtomicBoolean failed=new AtomicBoolean(false);
    DataNodeFaultInjector.set(new DataNodeFaultInjector(){
      @Override public void failPipeline(      ReplicaInPipelineInterface replicaInfo,      String mirror) throws IOException {
        if (!lastDn.equals(mirror)) {
          return;
        }
        if (!failed.get() && (replicaInfo.getBytesAcked() > errorInjectionPos) && (replicaInfo.getBytesAcked() % chunkSize != 0)) {
          int count=0;
          while (count < 10) {
            if ((replicaInfo.getBytesOnDisk() / chunkSize) - (replicaInfo.getBytesAcked() / chunkSize) >= 1) {
              failed.set(true);
              throw new IOException("Failing Pipeline " + replicaInfo.getBytesAcked() + " : "+ replicaInfo.getBytesOnDisk());
            }
            try {
              Thread.sleep(200);
            }
 catch (            InterruptedException e) {
            }
            count++;
          }
        }
      }
    }
);
    Random r=new Random();
    byte[] b=new byte[oneWriteSize];
    while (count < totalSize) {
      r.nextBytes(b);
      o.write(b);
      count+=oneWriteSize;
      o.hflush();
    }
    assertTrue("Expected a failure in the pipeline",failed.get());
    DatanodeInfo[] newNodes=dfsO.getStreamer().getNodes();
    o.close();
    for (    DataNode d : cluster.getDataNodes()) {
      DataNodeTestUtils.triggerBlockReport(d);
    }
    List<DatanodeInfo> pipelineList=Arrays.asList(pipeline);
    DatanodeInfo newNode=null;
    for (    DatanodeInfo node : newNodes) {
      if (!pipelineList.contains(node)) {
        newNode=node;
        break;
      }
    }
    LOG.info("Number of nodes in pipeline: {} newNode {}",newNodes.length,newNode.getName());
    for (int i=0; i < newNodes.length; i++) {
      if (newNodes[i].getName().equals(newNode.getName())) {
        continue;
      }
      LOG.info("shutdown {}",newNodes[i].getName());
      cluster.stopDataNode(newNodes[i].getName());
    }
    DFSTestUtil.readFile(fs,fileName);
  }
  finally {
    DataNodeFaultInjector.set(old);
    cluster.shutdown();
  }
}
