{
  fileContents=AppendTestUtil.initBuffer(AppendTestUtil.FILE_SIZE);
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,2000);
  conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,2);
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,2);
  conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,30000);
  conf.setInt(HdfsClientConfigKeys.DFS_DATANODE_SOCKET_WRITE_TIMEOUT_KEY,30000);
  conf.setInt(DFSConfigKeys.DFS_DATANODE_HANDLER_COUNT_KEY,50);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
  cluster.waitActive();
  FileSystem fs=cluster.getFileSystem();
  try {
    for (int i=0; i < numberOfFiles; i++) {
      final int replication=AppendTestUtil.nextInt(numDatanodes - 2) + 1;
      Path testFile=new Path("/" + i + ".dat");
      FSDataOutputStream stm=AppendTestUtil.createFile(fs,testFile,replication);
      stm.close();
      testFiles.add(testFile);
    }
    workload=new Workload[numThreads];
    for (int i=0; i < numThreads; i++) {
      workload[i]=new Workload(cluster,i,appendToNewBlock);
      workload[i].start();
    }
    for (int i=0; i < numThreads; i++) {
      try {
        System.out.println("Waiting for thread " + i + " to complete...");
        workload[i].join();
        System.out.println("Waiting for thread " + i + " complete.");
      }
 catch (      InterruptedException e) {
        i--;
      }
    }
  }
  finally {
    fs.close();
    cluster.shutdown();
  }
  assertTrue("testComplexAppend Worker encountered exceptions.",globalStatus);
}
