{
  conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
  assertTrue("unexpected test data at " + testFilePath,testFileSize > firstSplitLength);
  String delimiter=conf.get("textinputformat.record.delimiter");
  byte[] recordDelimiterBytes=null;
  if (null != delimiter) {
    recordDelimiterBytes=delimiter.getBytes(StandardCharsets.UTF_8);
  }
  FileSplit split=new FileSplit(testFilePath,0,testFileSize,(String[])null);
  LineRecordReader reader=new LineRecordReader(conf,split,recordDelimiterBytes);
  LongWritable key=new LongWritable();
  Text value=new Text();
  int numRecordsNoSplits=0;
  while (reader.next(key,value)) {
    ++numRecordsNoSplits;
  }
  reader.close();
  split=new FileSplit(testFilePath,0,firstSplitLength,(String[])null);
  reader=new LineRecordReader(conf,split,recordDelimiterBytes);
  int numRecordsFirstSplit=0;
  while (reader.next(key,value)) {
    ++numRecordsFirstSplit;
  }
  reader.close();
  split=new FileSplit(testFilePath,firstSplitLength,testFileSize - firstSplitLength,(String[])null);
  reader=new LineRecordReader(conf,split,recordDelimiterBytes);
  int numRecordsRemainingSplits=0;
  while (reader.next(key,value)) {
    ++numRecordsRemainingSplits;
  }
  reader.close();
  assertEquals("Unexpected number of records in split",numRecordsNoSplits,numRecordsFirstSplit + numRecordsRemainingSplits);
}
