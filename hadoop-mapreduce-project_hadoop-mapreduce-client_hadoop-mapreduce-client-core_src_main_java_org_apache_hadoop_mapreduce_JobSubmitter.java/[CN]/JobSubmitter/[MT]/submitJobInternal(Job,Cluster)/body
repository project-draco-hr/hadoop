{
  checkSpecs(job);
  Configuration conf=job.getConfiguration();
  addMRFrameworkToDistributedCache(conf);
  Path jobStagingArea=JobSubmissionFiles.getStagingDir(cluster,conf);
  InetAddress ip=InetAddress.getLocalHost();
  if (ip != null) {
    submitHostAddress=ip.getHostAddress();
    submitHostName=ip.getHostName();
    conf.set(MRJobConfig.JOB_SUBMITHOST,submitHostName);
    conf.set(MRJobConfig.JOB_SUBMITHOSTADDR,submitHostAddress);
  }
  JobID jobId=submitClient.getNewJobID();
  job.setJobID(jobId);
  Path submitJobDir=new Path(jobStagingArea,jobId.toString());
  JobStatus status=null;
  try {
    conf.set(MRJobConfig.USER_NAME,UserGroupInformation.getCurrentUser().getShortUserName());
    conf.set("hadoop.http.filter.initializers","org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer");
    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR,submitJobDir.toString());
    LOG.debug("Configuring job " + jobId + " with "+ submitJobDir+ " as the submit dir");
    TokenCache.obtainTokensForNamenodes(job.getCredentials(),new Path[]{submitJobDir},conf);
    populateTokenCache(conf,job.getCredentials());
    if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {
      KeyGenerator keyGen;
      try {
        keyGen=KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);
        keyGen.init(SHUFFLE_KEY_LENGTH);
      }
 catch (      NoSuchAlgorithmException e) {
        throw new IOException("Error generating shuffle secret key",e);
      }
      SecretKey shuffleKey=keyGen.generateKey();
      TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),job.getCredentials());
    }
    if (CryptoUtils.isEncryptedSpillEnabled(conf)) {
      conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS,1);
      LOG.warn("Max job attempts set to 1 since encrypted intermediate" + "data spill is enabled");
    }
    copyAndConfigureFiles(job,submitJobDir);
    Path submitJobFile=JobSubmissionFiles.getJobConfPath(submitJobDir);
    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
    int maps=writeSplits(job,submitJobDir);
    conf.setInt(MRJobConfig.NUM_MAPS,maps);
    LOG.info("number of splits:" + maps);
    int maxMaps=conf.getInt(MRJobConfig.JOB_MAX_MAP,MRJobConfig.DEFAULT_JOB_MAX_MAP);
    if (maxMaps >= 0 && maxMaps < maps) {
      throw new IllegalArgumentException("The number of map tasks " + maps + " exceeded limit "+ maxMaps);
    }
    String queue=conf.get(MRJobConfig.QUEUE_NAME,JobConf.DEFAULT_QUEUE_NAME);
    AccessControlList acl=submitClient.getQueueAdmins(queue);
    conf.set(toFullPropertyName(queue,QueueACL.ADMINISTER_JOBS.getAclName()),acl.getAclString());
    TokenCache.cleanUpTokenReferral(conf);
    if (conf.getBoolean(MRJobConfig.JOB_TOKEN_TRACKING_IDS_ENABLED,MRJobConfig.DEFAULT_JOB_TOKEN_TRACKING_IDS_ENABLED)) {
      ArrayList<String> trackingIds=new ArrayList<String>();
      for (      Token<? extends TokenIdentifier> t : job.getCredentials().getAllTokens()) {
        trackingIds.add(t.decodeIdentifier().getTrackingId());
      }
      conf.setStrings(MRJobConfig.JOB_TOKEN_TRACKING_IDS,trackingIds.toArray(new String[trackingIds.size()]));
    }
    ReservationId reservationId=job.getReservationId();
    if (reservationId != null) {
      conf.set(MRJobConfig.RESERVATION_ID,reservationId.toString());
    }
    writeConf(conf,submitJobFile);
    Limits.reset(conf);
    printTokens(jobId,job.getCredentials());
    status=submitClient.submitJob(jobId,submitJobDir.toString(),job.getCredentials());
    if (status != null) {
      return status;
    }
 else {
      throw new IOException("Could not launch job");
    }
  }
  finally {
    if (status == null) {
      LOG.info("Cleaning up the staging area " + submitJobDir);
      if (jtFs != null && submitJobDir != null)       jtFs.delete(submitJobDir,true);
    }
  }
}
