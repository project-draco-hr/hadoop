{
  final JobConf conf=mrCluster.createJobConf();
  conf.setJobName("[name][some other value that gets" + " truncated internally that this test attempts to aggravate]");
  conf.setInputFormat(TextInputFormat.class);
  conf.setOutputFormat(TextOutputFormat.class);
  conf.setMapOutputKeyClass(LongWritable.class);
  conf.setMapOutputValueClass(Text.class);
  conf.setOutputKeyClass(LongWritable.class);
  conf.setOutputValueClass(Text.class);
  conf.setCompressMapOutput(true);
  conf.setMapperClass(TestUserDefinedCounters.CountingMapper.class);
  conf.set("mapred.reducer.class","testjar.ExternalIdentityReducer");
  conf.setLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE,1024 * 1024);
  conf.setNumReduceTasks(numReduces);
  conf.setJobPriority(JobPriority.HIGH);
  conf.setJar("build/test/mapred/testjar/testjob.jar");
  String pattern=TaskAttemptID.getTaskAttemptIDsPattern(null,null,TaskType.MAP,1,null);
  conf.setKeepTaskFilesPattern(pattern);
  final Path inDir=new Path("./test/input");
  final Path outDir=new Path("./test/output");
  TEST1_UGI.doAs(new PrivilegedExceptionAction<Void>(){
    public Void run(){
      FileInputFormat.setInputPaths(conf,inDir);
      FileOutputFormat.setOutputPath(conf,outDir);
      return null;
    }
  }
);
  clean(fs,outDir);
  final RunningJob job=TEST1_UGI.doAs(new PrivilegedExceptionAction<RunningJob>(){
    public RunningJob run() throws IOException {
      makeInput(inDir,conf);
      JobClient jobClient=new JobClient(conf);
      return jobClient.submitJob(conf);
    }
  }
);
  final JobID jobId=job.getID();
  while (job.getJobState() != JobStatus.RUNNING) {
    try {
      Thread.sleep(100);
    }
 catch (    InterruptedException e) {
      break;
    }
  }
  assertFalse("Missing event notification for a running job",myListener.contains(jobId,true));
  job.waitForCompletion();
  assertTrue(job.isComplete());
  assertEquals(JobStatus.SUCCEEDED,job.getJobState());
  assertFalse("Missing event notification for a successful job",myListener.contains(jobId,false));
  TaskAttemptID taskid=new TaskAttemptID(new TaskID(jobId,TaskType.MAP,1),0);
  TestMiniMRWithDFS.checkTaskDirectories(mrCluster,TEST1_UGI.getUserName(),new String[]{jobId.toString()},new String[]{taskid.toString()});
  ByteArrayOutputStream out=new ByteArrayOutputStream();
  int exitCode=TestJobClient.runTool(conf,new JobClient(),new String[]{"-counter",jobId.toString(),"org.apache.hadoop.mapreduce.TaskCounter","MAP_INPUT_RECORDS"},out);
  assertEquals(0,exitCode);
  assertEquals(numReduces,Integer.parseInt(out.toString().trim()));
  TestUserDefinedCounters.verifyCounters(job,numTT);
  TestJobClient.verifyJobPriority(jobId.toString(),"HIGH",conf);
  TEST1_UGI.doAs(new PrivilegedExceptionAction<Void>(){
    public Void run() throws IOException {
      verifyOutput(outDir.getFileSystem(conf),outDir);
      TestJobHistory.validateJobHistoryFileFormat(mrCluster.getJobTrackerRunner().getJobTracker().getJobHistory(),jobId,conf,"SUCCEEDED",false);
      TestJobHistory.validateJobHistoryFileContent(mrCluster,job,conf);
      for (int i=0; i < numTT; ++i) {
        Path jobDirPath=new Path(mrCluster.getTaskTrackerLocalDir(i),TaskTracker.getJobCacheSubdir(TEST1_UGI.getUserName()));
        boolean b=FileSystem.getLocal(conf).delete(jobDirPath,true);
        assertTrue(b);
      }
      return null;
    }
  }
);
}
