{
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
  conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
  if (simulatedStorage) {
    conf.setBoolean(SimulatedFSDataset.CONFIG_PROPERTY_SIMULATED,true);
  }
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  cluster.waitActive();
  InetSocketAddress addr=new InetSocketAddress("localhost",cluster.getNameNodePort());
  DFSClient client=new DFSClient(addr,conf);
  try {
    Path file1=new Path("/filestatus.dat");
    FSDataOutputStream stm=createFile(fs,file1,1);
    assertTrue(file1 + " should be a file",fs.getFileStatus(file1).isFile());
    System.out.println("Path : \"" + file1 + "\"");
    cluster.shutdownDataNodes();
    while (true) {
      DatanodeInfo[] info=client.datanodeReport(FSConstants.DatanodeReportType.LIVE);
      if (info.length == 0) {
        break;
      }
      System.out.println("testFileCreationError1: waiting for datanode " + " to die.");
      try {
        Thread.sleep(1000);
      }
 catch (      InterruptedException e) {
      }
    }
    byte[] buffer=AppendTestUtil.randomBytes(seed,1);
    try {
      stm.write(buffer);
      stm.close();
    }
 catch (    Exception e) {
      System.out.println("Encountered expected exception");
    }
    LocatedBlocks locations=client.getNamenode().getBlockLocations(file1.toString(),0,Long.MAX_VALUE);
    System.out.println("locations = " + locations.locatedBlockCount());
    assertTrue("Error blocks were not cleaned up",locations.locatedBlockCount() == 0);
  }
  finally {
    cluster.shutdown();
    client.close();
  }
}
